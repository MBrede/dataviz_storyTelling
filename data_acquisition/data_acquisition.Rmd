<!-- # --- -->
<!-- title: "acquisition" -->
<!-- output: -->
<!--   # pdf_document: -->
<!--   #   includes: -->
<!--   #     in_header: header_doc.tex -->
<!--   html_document: -->
<!--     css: styles.css -->
<!--     fig_caption: true -->
<!--     toc: true -->
<!-- csl: apa.csl -->
<!-- bibliography: references.bib -->
<!-- --- -->
# Data Acquisition

```{r setup acq, include=FALSE}
library(knitr) # import of stuff
library(kableExtra) # nice tables
library(tidyverse) # general handling

library(glue) # for text-string handling
library(slam) # for memory efficient matrix-representations
library(magrittr) #for additional pipes
library(furrr) # parallel-processing


opts_chunk$set(echo = T,message = F, fig.align = 'center')

theme_set(theme_light())

```

## Sources for News-Articles {.tabset}


We want to take a look at the German news reports about the Swedish response to the COVID-19 Pandemic and vice versa.

To do this, we will use the [Mediacloud](https://mediacloud.org/#/Home) API to gather news articles for the time frame since January of 2020. This API is publicly available after a free registration.

The API allows users to search for terms for a given set of news-media outlets in addition to some filter settings like a given time frame and such. These queries can be sent to a whole range of API endpoints, including one that returns document term matrices and one that returns all gathered meta-information MediaCloud collected.

Since we only want to look at german and swedish media outlets, we will be using two "Collections" of outlets as provided by Mediacloud. The selections we will be using are discussed in the following section.


### Sweden
We will use a collection of [Swedish](https://sources.mediacloud.org/#/collections/34412223) news-outlets with focus on local and national news as provided by Mediacloud. 

Since we do not have the necessary domain-knowledge to judge the validity of this selection, we will use it unchanged except for a filter that removes any source who's main language is not swedish. The resulting selection consists of the outlets listed in table 1.

```{r, eval = T}
swe_sources <- read_csv('./../data/sweden.csv') %>% # Sources of Swedish news
  filter(language == 'sv') %>% 
  select(media_id, url, name,stories_per_day, pub_state)

swe_sources %>% 
  select(title = name,
         URL = url,
         `average number of articles per day` = stories_per_day) %>% 
  arrange(desc(`average number of articles per day`)) %>% 
  DT::datatable(caption = 'Table 1: the selection of Swedish news-outlets used in our analysis.')

```



### Germany

We will use a collection of [German](https://sources.mediacloud.org/#/collections/38379816) news-outlets with focus on local and national news as provided by Mediacloud. 

After a superficial check of the sets validity, the presence of Russian outlets became aparent. But since these were correctly marked as being in Russian, they could be easily removed by filtering for German sources only. 

We additionaly realized, that the Spiegel as one of Germanys largest outlets is missing from the collection, so we added it after searching the Mediacloud-Page for it's ID.

```{r, eval = T}
ger_sources <- read_csv('./../data/germany.csv') %>% # Sources of German News
  filter(language == 'de') %>%
  select(media_id, url, name, stories_per_day, pub_state) %>%
  bind_rows(tibble(media_id = 19831,
                   url = 'https://www.spiegel.de',
                   name = 'der Spiegel',
                   stories_per_day = 82,
                   pub_state = NA))
  
ger_sources %>% 
  select(title = name,
         URL = url,
         `average number of articles per day` = stories_per_day) %>% 
  arrange(desc(`average number of articles per day`)) %>% 
  DT::datatable(caption = 'Table 2: the selection of German news-outlets used in our analysis.')
```




```{r, include=F, eval = F}
save(list = ls(),file = './../data/pre_steps.RData')
```


## Data Collection

We then use the collections to query the API for the matrices and the story-meta-information. To do this, we first build our query-function using httr:

```{r, eval = F}

get_stories <- function(time_range,query,media_ids){
  require(httr) # GET and POST-requests
  
  base_url <- 'https://api.mediacloud.org/api/v2/stories_public/' # basic API-URL
  
  query_string <- glue('?q=text:', query) # add the query
  
  url <- glue(base_url,'word_matrix', query_string, '&key=', mc_key) # add the endpoint and the API-key
  
  resp <-
    POST(
      glue(
        url,
        '&publish_day=',time_range, # add time range
        '&rows=2000&stopword_length=long' # add the short filters
      ),
      body = jsonlite::toJSON(list(fq = list(media_id = media_ids))) # add the longer filters
    ) # POST the request
  
  cont <- content(resp) # get the response
  
  story_ids <- names(cont$word_matrix) # extract the row-indices to get the meta-data
  
  get_story <- function(story_id){ # helper function to query every story-id
    url <- glue(base_url,'single/', story_id, '?key=', mc_key) # Endpoint and API-key
    content(GET(url)) %>%  # Get the Meta-info and format it prettier
      bind_cols() %>% 
      mutate(across(everything(),
                    ~as.character(.)))
  }
  
  v <- lapply(cont$word_matrix, unlist) # count per document as list of respective vectors
  
  r_names <- story_ids # doc-names for our doc-term matrix
  
  c_names <- map_chr(cont$word_list,
                     ~nth(.,2)) # get the first non-stemmed term as the words for the doc-term matrix
  
  if(length(v)>0){ # check if there even is any word, i.e. if any article was collected
    list( # return a list with
      triplet = # a simple triplet that indicates every nun-zero combination of words and documents
        simple_triplet_matrix(
          i = unlist(map(seq_along(v),  ~ rep(., length(
            v[[.]]
          ))), use.names = F),
          j = unlist(map(v,  ~ as.numeric(names(
            .
          ))), use.names = F) + 1,
          v = unlist(v, use.names = F),
          dimnames = list(rows = r_names,
                          cols = c_names)
        ),
      story_info = # and a tibble with all meta-information
        story_ids %>%
        map_dfr( ~ get_story(.))
    )
  }else{
    return(list()) # if there is no word in the matrix return an empty list
  }
}

```

Since we are limiting every query to 2000 storys to reduce network-strain, we will send a query for every week in 2020. We therefore construct a vector of date-ranges.

```{r, eval = F}
weeks <- paste0('[',
                as.Date('2020-01-01') +lubridate::weeks(c(0:51)),
                'T00:00:00Z TO ',
                as.Date('2020-01-01') +lubridate::weeks(c(1:52)),
                'T00:00:00Z]')

```

The last step is to define the actual query-terms. Since we want to see only the reportings about corona in which the respective other country is mentioned, we settled on the boolean combination of terms for corona and terms for the other nationality, i.e. `(covid OR corona) AND (tyskland OR tysk)` for Sweden and `(covid OR corona) AND (schweden OR schwedisch)` for Germany.

Now everything is prepared to run our queries. Since we wanted it to run quickly, we cheated a bit and ran it in a separate script, but we used the following code:

```{r, eval = F}

plan(multicore(workers = 8))

swedish_stories <- weeks %>%
  future_map( ~ get_stories(
    .,
    '(covid OR corona) AND (tyskland OR tysk)',
    swe_sources$media_id
  ),.progress = T)

german_stories <- weeks %>%
  future_map( ~ get_stories(
    .,
    '(covid OR corona) AND (schweden OR schwedisch)',
    ger_sources$media_id
  ), .progress = T)
```



```{r, include=F, eval = F}
load('./../data/collected_data.RData')
```

## Combining the results {.tabset}

The first step in processing the data is to remove all empty lists that indicate weeks with no articles whatsoever. This can be achieved with a simple filter:

```{r, eval = F}

german_stories %<>% .[which(map_lgl(.,  ~ length(.) > 1))]
swedish_stories %<>% .[which(map_lgl(.,  ~ length(.) > 1))]

```

We then need to do some processing to end up with the datasets that we need.
Since we used forked processing to gather our data, we have to combine it. The aim is to end up with four objects; one meta-tibble and one document-term matrix for each country.

### Meta-data

To aggregate our our meta-datasets we first need to define a few helper-functions which mostly do a bit of cleaning since the resulting data-frames are a bit in-consistent.

```{r, eval = F}

first_non_na <- function(row){
  first(row[which(!is.na(row))])
} # a simple helper to reduce the strange amount of empty story-id columns

extract_meta <- function(stories){ # a cleaner-function to end up with one streamlined meta-dataset for each country
  stories %>%
    map_dfr( ~ nth(., 2) %>%
               select(publish_date, guid, title, url,media_id, matches('stories_id'))) %>%
    rowwise() %>%
    mutate(stories_id = first_non_na(c_across(matches('stories')))) %>%
    select(-matches('stories_id.+')) %>%
    distinct
}

german_meta <- german_stories %>% 
  extract_meta() %T>%
  write_rds('./../data/german_meta.rds') #save for later

swedish_meta <- swedish_stories %>% 
  extract_meta() %T>%
  write_rds('./../data/swedish_meta.rds')#save for later

```

### Document-Term-Matrices

To combine the triplet-matrices, we need to define some helpers again. A simple row-binding of the matrices would not do, since every set of articles will have a seperate set of words. So we will need to use some R-trickery.

```{r, eval = F}

resolve_redundancies <- function(mat1,mat2=NULL){ #combines two triplets into one
  if(is.null(mat2)){
    input <- list(mat1)
  }else{
    input <- list(mat1, mat2)
  }
  word_tibble <- input %>%
    map_dfr(~ tibble(
      story = .$dimnames$rows[.$i],
      word = .$dimnames$cols[.$j],
      count = .$v
    )) %>%
    mutate(i = as.numeric(as.factor(story)),
           j = as.numeric(as.factor(word)))
    return(simple_triplet_matrix(word_tibble$i,
                                 word_tibble$j,
                                 word_tibble$count,
                                 dimnames = list(rows = sort(unique(word_tibble$story)),
                                                 cols = sort(unique(word_tibble$word)))))
  
}

combine_all_triplets <- function(triplets, language){ # iterative wrapper to combine all sets
  require(SnowballC)
  out <-  triplets[[1]]
  for(i in 2:length(triplets)){
    out <- resolve_redundancies(out, triplets[[i]])
  }
  # finally we'll stem it but save the original words
  
  stem_lex <- tibble(long_ver = out$dimnames$cols,
                     stems = wordStem(long_ver, language = language)) %>% 
    nest_by(stems)
  out$dimnames$cols %<>% wordStem(language = language)
  out %<>% resolve_redundancies()
  return(list(dtm = out, stem_lex = stem_lex))
}
```

These helpers can then be used to assemble our final matrices.

```{r, eval = F}
german_doc_term <- german_stories %>% 
  map(~first(.)) %>% 
  combine_all_triplets('german')


write_rds(first(german_doc_term),
            './../data/german_dtm.rds',
            compress = 'gz') #save for later

write_rds(nth(german_doc_term,2),
            './../data/german_stem_keys.rds',
            compress = 'gz')



swedish_doc_term <- swedish_stories %>% 
  map(~first(.)) %>% 
  combine_all_triplets('swedish') 


write_rds(first(swedish_doc_term),
            './../data/swedish_dtm.rds',
            compress = 'gz') #save for later

write_rds(nth(swedish_doc_term,2),
            './../data/swedish_stem_keys.rds',
            compress = 'gz')
```


## Pre-processing for the text visualisations

To end up with some nice and sensible results, we need to do some additional text-cleaning.
The first step is to remove all unnecessary words, since there are a lot of in our context 'filler-words that are usefull for the reader but useless for our visualisations.

As usual, we start by defining a helper-function to do our cleaning. We went with quite a radical solution, removing all words shorter than 4 symbols, all character-strings with non-letter contents and a high cutoff for the term frequency - inverse document frequency value, a value indicating how important each word is to individual documents in the corpus. This indicator is the product of the average term-frequency $$tf_t = {1\over N_d}\sum_{d=1}^{N_d} tf_{t, d}$$ where $N_d$ is the total amount of documents in the corpus and  $$tf_{t, d} = {f_{t,d}\over{\sum_{i=1}^{N_t}f_{i, d}}}$$ where $t$ is a given term, $N_t$ is the total number of terms in the corpus and $f_{t,d}$ is the frequency of a given term in a given document with the inverse document frequency $$idf_{t,D} = \log{ {N_d}\over{|\{d \in D : t \in d\}|}}$$ where  $|\{d \in D : t \in d\}|$ is the number of documents that mention the respective term at least once.
 

```{r, eval = T}

remove_stoppers <- function(dtm, language = 'german'){
  require(tm) # for the stopwords
  require(SnowballC)
  
  log_idx <- str_detect(dtm$dimnames$cols,'[^[:alpha:]]', negate = T) & 
             str_count(dtm$dimnames$cols) > 3 &
             !(dtm$dimnames$cols %in% wordStem(stopwords(language),language = language))
  
  dtm %<>% .[,log_idx]
  
  TF_IDF <-  tapply(dtm$v/row_sums(dtm)[dtm$i], dtm$j, mean) *  log(dtm$nrow/col_sums(dtm > 0))
  dtm %<>% .[,TF_IDF > 1.75 * median(TF_IDF)] #arbitrary cutoff
  dtm
}
```

This filter-function is then applied to both matrices:

```{r, eval = T}
swedish_dtm <- read_rds('./../data/swedish_dtm.rds') %>% 
  remove_stoppers(language = 'swedish')

german_dtm <- read_rds('./../data/german_dtm.rds') %>% 
  remove_stoppers(language = 'german')
```
## Sentiments{.tabset}

For the second of our visualisations we now need to  determine the sentiment for the terms left in our corpus. 

### Swedish

For the Swedish set, we will use a dictionary provided by github user [AlexGustafsson](https://github.com/AlexGustafsson) which is available [here](https://github.com/AlexGustafsson/sentiment-swedish/blob/develop/build/AFINN-sv-165.txt).

```{r, eval = T}
swedish_word_freq <- swedish_dtm %$%
  tibble(word = dimnames$cols[j],
         n = v) %>% 
  group_by(word) %>% 
  summarise(n = sum(n))

swedish_word_freq %>% 
  skimr::skim()

swe_sent <- read_lines('https://raw.githubusercontent.com/AlexGustafsson/sentiment-swedish/develop/build/AFINN-sv-165.txt')

swe_sent <- tibble(
  word = str_remove(swe_sent, '\\s\\-?\\d$'),
  sentiment = as.numeric(str_extract(swe_sent, '\\-?\\d$'))) %>% 
  mutate(word = wordStem(word, language = 'swedish')) %>% 
  group_by(word) %>% 
  summarise(sentiment = mean(sentiment)) %>% 
  right_join(swedish_word_freq,by = c('word' = 'word')) 

swe_sent %>% 
  skimr::skim()


```

### German
For the German sentiments, we will use the SentiWS set [@remusSentiWSAPubliclyAvailable2010], as obtained from [here](https://www.kaggle.com/rtatman/german-sentiment-analysis-toolkit).

```{r, eval = T}
 german_word_freq<- german_dtm %$%
  tibble(word = dimnames$cols[j],
         n = v) %>% 
  group_by(word) %>% 
  summarise(n = sum(n))


ger_sent <- read_tsv('./../data/german_sentis.txt', col_names = F) %>% 
  select(1:2) %>% 
  set_names(c('word', 'sentiment')) %>% 
  mutate(word = str_remove(word, '\\|.+$'),
         word = str_to_lower(word),
         word = wordStem(word, language = 'german')) %>% 
  group_by(word) %>% 
  summarise(sentiment = mean(sentiment)) %>%
  right_join(german_word_freq,by = c('word' = 'word')) 


ger_sent %>% 
  skimr::skim()
```


## Translation

```{r, include= F, eval = F}
save(german_word_freq, swedish_word_freq, file = './../data/word_freqs.RData')
```


Since the non-english dictionaries we found are rather uncomplete, we'll try one version with translated words. This does mean though, that we can only look at a limited amout of terms since the translation-API we use is limited.

Let's first define a helper function to use the [MyMemory](https://mymemory.translated.net/doc/spec.php) translation API.

```{r, eval = F}
translate_with_mymemory <- function(word, from, to = 'en'){
  .GlobalEnv$translated <- 0
  require(httr)
  ret <- GET(glue('https://api.mymemory.translated.net/get?q=',
                  word, '&langpair=',
                  from, '|', to,
                  '&de=max.brede@student.fh-kiel.de'))
  out <- tryCatch({
    content(ret)$matches %>%
      map_dfr( ~ bind_cols(.) %>%  mutate(across(everything(), ~ as.character(.)))) %>%
      dplyr::slice(which.max(quality)) %>%
      select(translation, quality) %>%
      transmute(word = word,
                translation,
                quality)
  },
  error = function(e) {
    word
  })
  return(out)
  
}
```


We'll then use this helper to translate the 5000 most used words in both matrices. (We used an external script again, but the code ist the same.)

```{r, eval = F}
plan(multicore(workers = 8))

german_keys <- read_rds('./../data/german_stem_keys.rds') 

german_word_freq %<>% 
  arrange(desc(n)) %>% 
  slice(1:5000) %>%
  left_join(german_keys, by = c('word' = 'stems')) %>% 
  transmute(word,n,long = map_chr(data,~first(unlist(.)))) %>% 
  mutate(tranlation = future_map(long, ~translate_with_mymemory(.,'de'),.progress = T))

german_word_freq %>% 
  write_rds('./../data/german_translated.rds', compress = 'gz')



swedish_keys <- read_rds('./../data/swedish_stem_keys.rds') 

swedish_word_freq %<>% 
  arrange(desc(n)) %>% 
  slice(1:5000) %>%
  left_join(swedish_keys, by = c('word' = 'stems')) %>% 
  transmute(word,n,long = map_chr(data,~first(unlist(.)))) %>% 
  mutate(tranlation = future_map(long, ~translate_with_mymemory(.,'sv'), .progress = T))

swedish_word_freq %>% 
  write_rds('./../data/swedish_translated.rds', compress = 'gz')
```


Let's now see, how good the API really is (after doing some minor cleaning).

```{r, eval =T}

clean_myMemory <- function(translated_set){
  translated_set %>% 
    transmute(stem = `word`,
              long,
              translation = map_chr(tranlation,~tryCatch(.$translation, error = function(e){.})),
              translation = str_to_lower(translation),
              translation = str_remove_all(translation,'\\W'),
              translation = str_trim(translation),
              n) 
}

ger_tranlated <- read_rds('./../data/german_translated.rds') %>% 
  clean_myMemory() #%T>% 
  #write_csv('data/manually_cleaned_german.csv')
  
DT::datatable(ger_tranlated)

swe_translated <-  read_rds('./../data/swedish_translated.rds')%>% 
  clean_myMemory() #%T>%
  #write_csv('data/manually_cleaned_swedish.csv') 

DT::datatable(swe_translated)
```

Unfortunately, the quality of the translation is quite inconsistent. To tackle that, we conducted a manual correction of the 250 most common terms resulting csv in Excel.

The last step is to

  1. assign sentiment values to the keys
  2. come back to the documents and add the sentiments to our meta data-frame
  
We will try to augment the English approach with the multilingual one but the results will be far from perfect.

So let's first add sentiments using the sentiments as indicated in the [AFINN-set](http://www2.imm.dtu.dk/pubdb/pubs/6010-full.html) [@nielsenNewANEWEvaluation2011], (though it was originally intended for microblogging) and the `sentiments`-df from the `tidytext` package, which is the set as reported by @huMiningSummarizingCustomer2004. Since the sentiments in the latter are only indicated as 'positive' and 'negative', we need to transform this to the endpoints of the AFINN-set (-5,5). This is obviously a heavily skewed approach, but we need to accept imperfections at some point. We will use the higher resoluting AFINN-sentiments where we can though.

```{r, eval = T}

add_sentiments <- function(translated_set){
  translated_set%>% 
    left_join(read_tsv('./../data/AFINN-111.txt', col_names = c('term', 'sentiment')),
              by = c('translation' = 'term')) %>% 
    left_join(tidytext::sentiments, 
              by = c('translation' = 'word')) %>% 
    rename(sentiment_AFINN = sentiment.x, sentiment_tidytext = sentiment.y) %>% 
    mutate(sentiment_tidytext = recode(sentiment_tidytext, "positive" = 5, 'negative' = -5),
           sentiment = ifelse(is.na(sentiment_AFINN), sentiment_tidytext, sentiment_AFINN))
}

ger_tranlated <- read_csv('./../data/manually_cleaned_german.csv') %>% 
  add_sentiments()

swe_translated <- read_csv('./../data/manually_cleaned_swedish.csv') %>% 
  add_sentiments()

```
The next step is to now combine these results with the multilingual approach (after scaling the German sentiments from that approach to the -5:5 scale). Since the two sentiment measurements are bound to differ, we'll average them wherever there are two values

```{r, eval = T, cache=T}
ger_sent %<>% 
  mutate(sentiment = sentiment/max(abs(sentiment),na.rm = T) * 5) %>% 
  left_join(ger_tranlated[,c('stem','translation', 'sentiment')], by = c('word' = 'stem')) %>% 
  rowwise() %>% 
  mutate(sentiment = mean(c_across(matches('sentiment')), na.rm = T)) %>% 
  select(word, translation, sentiment, n)
  

skimr::skim(ungroup(ger_sent))


swe_sent %<>% 
  left_join(swe_translated[,c('stem','translation', 'sentiment')], by = c('word' = 'stem')) %>% 
  rowwise() %>% 
  mutate(sentiment = mean(c_across(matches('sentiment')), na.rm = T)) %>% 
  select(word,translation, sentiment, n)

skimr::skim(ungroup(swe_sent))

```

## Merging and exporting the final sets

For the two visualisations we had in mind we'll need two data sets. 

1. the overall count and sentiment per country

2. the timecourse for each country, including sentiments and article-counts


The first one is easy, we just need to combine the sentiment sets we created in the last step:

```{r, eval = F}
imap_dfr(list('Sweden' = swe_sent,
              'Germany' = ger_sent),
          ~mutate(.x, country = .y)) %>% 
  write_rds('./../data/wordcloud_data.rds',compress = 'gz')
```

The second one is a bit more tricky, since we need to look up in the respective dtms, which document used which terms in which frequency to then average the respective sentiments. This sentiment-per-document-set can then be joined with the existing meta-data.

```{r, eval = F}
get_sentiment_per_document <- function(dtm, sentiment){
  
  extract_counts <- function(id,dtm, sentiment){
    tibble(n = dtm$v[dtm$i == which(dtm$dimnames$rows == id)],
           term = dtm$dimnames$cols[dtm$j[dtm$i == which(dtm$dimnames$rows == id)]]) %>% 
      left_join(sentiment[,1:2], by = c('term' = 'word')) %>% 
      mutate(partial_sent = n * sentiment/sum(n)) %$% 
      tibble(id = id,
             m_sentiment = sum(sentiment, na.rm = T))
  }
  
  map_dfr(dtm$dimnames$rows, ~extract_counts(., dtm, sentiment))
}


swe_sent_per_doc <- get_sentiment_per_document(swedish_dtm, swe_sent)
ger_sent_per_doc <- get_sentiment_per_document(german_dtm, ger_sent)


ger_part <- ger_sent_per_doc %>% 
  mutate(country = 'Germany') %>% 
  right_join(read_rds('./../data/german_meta.rds'), by=c('id' = 'stories_id')) %>% 
  mutate(media_id = as.numeric(media_id)) %>% 
  left_join(ger_sources[,c('media_id', 'pub_state')]) %>% 
  select(country, title,m_sentiment, publish_date, everything())
  
swe_part <- swe_sent_per_doc %>% 
  mutate(country = 'Sweden') %>% 
  right_join(read_rds('./../data/swedish_meta.rds'), by=c('id' = 'stories_id')) %>% 
  mutate(media_id = as.numeric(media_id)) %>% 
  left_join(swe_sources[,c('media_id', 'pub_state')]) %>% 
  select(country, title,m_sentiment, publish_date, everything())

bind_rows(ger_part, swe_part) %>% 
  write_rds('./../data/documents_with_sentiments.rds', compress = 'gz')



```





<!-- ## References -->
