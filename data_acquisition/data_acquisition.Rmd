<!-- # --- -->
<!-- title: "acquisition" -->
<!-- output: -->
<!--   # pdf_document: -->
<!--   #   includes: -->
<!--   #     in_header: header_doc.tex -->
<!--   html_document: -->
<!--     css: styles.css -->
<!--     fig_caption: true -->
<!--     toc: true -->
<!-- csl: apa.csl -->
<!-- bibliography: references.bib -->
<!-- --- -->
# Data Acquisition

```{r setup acq, include=FALSE}
library(knitr) # import of stuff
library(kableExtra) # nice tables
library(tidyverse) # general handling

library(glue) # for text-string handling
library(slam) # for memory efficient matrix-representations
library(magrittr) #for additional pipes
library(furrr) # parallel-processing


opts_chunk$set(echo = T,message = F, fig.align = 'center')

theme_set(theme_light())

```

## Sources for News-Articles {.tabset}


We want to analyse the German news reports about the Swedish response to the COVID-19 pandemic and vice versa.

To do this, we will use the [Mediacloud](https://mediacloud.org/#/Home) API to gather news-articles for the time frame since January of 2020. The interface is publicly available after a free registration.

The API allows users to search for terms for a given set of news media outlets in addition to some filter settings like a given time frame and such. These queries can be sent to a whole range of API endpoints, including one that returns document term matrices and one that returns all gathered metadata MediaCloud collected.

Since we only want to look at German and Swedish media outlets, we will be using two "Collections" of outlets as provided by Mediacloud. The selections we will be using are discussed in the following section.


### Sweden
We will use a collection of [Swedish](https://sources.mediacloud.org/#/collections/34412223) news-outlets with focus on local and national news as provided by Mediacloud. 

Since we do not have the necessary domain knowledge to judge the validity of this selection, we will use it unchanged except for a filter that removes any source with a main language other than Swedish. The resulting selection consists of the outlets listed in table 1.

```{r, eval = T}
swe_sources <- read_csv('./../data/sweden.csv') %>% # Sources of Swedish news
  filter(language == 'sv') %>% 
  select(media_id, url, name,stories_per_day, pub_state)

swe_sources %>% 
  select(title = name,
         URL = url,
         `average number of articles per day` = stories_per_day) %>% 
  arrange(desc(`average number of articles per day`)) %>% 
  DT::datatable(caption = 'Table 1: The selection of Swedish news outlets used in our analysis.')

```



### Germany

We will use a collection of [German](https://sources.mediacloud.org/#/collections/38379816) news outlets with focus on local and national news as provided by Mediacloud. 

After a superficial check of the set's validity, the presence of Russian outlets became apparent. But since these were correctly marked as being in Russian, they could easily be removed by filtering for German sources only. 

We additionally realized, that the Spiegel as one of Germany's largest outlets is missing from the collection, so we added it after searching the Mediacloud-Page for it's ID.

```{r, eval = T}
ger_sources <- read_csv('./../data/germany.csv') %>% # Sources of German News
  filter(language == 'de') %>%
  select(media_id, url, name, stories_per_day, pub_state) %>%
  bind_rows(tibble(media_id = 19831,
                   url = 'https://www.spiegel.de',
                   name = 'der Spiegel',
                   stories_per_day = 82,
                   pub_state = NA))
  
ger_sources %>% 
  select(title = name,
         URL = url,
         `average number of articles per day` = stories_per_day) %>% 
  arrange(desc(`average number of articles per day`)) %>% 
  DT::datatable(caption = 'Table 2: The selection of German news outlets used in our analysis.')
```




```{r, include=F, eval = F}
save(list = ls(),file = './../data/pre_steps.RData')
```


## News Articles

We then use the collections to query the API for the matrices and the stories' metadata. To do this, we first build our query-function using `httr`:

```{r, eval = F}

get_stories <- function(time_range,query,media_ids){
  require(httr) # GET and POST-requests
  
  base_url <- 'https://api.mediacloud.org/api/v2/stories_public/' # basic API-URL
  
  query_string <- glue('?q=text:', query) # add the query
  
  url <- glue(base_url,'word_matrix', query_string, '&key=', mc_key) # add the endpoint and the API-key
  
  resp <-
    POST(
      glue(
        url,
        '&publish_day=',time_range, # add time range
        '&rows=2000&stopword_length=long' # add the short filters
      ),
      body = jsonlite::toJSON(list(fq = list(media_id = media_ids))) # add the longer filters
    ) # POST the request
  
  cont <- content(resp) # get the response
  
  story_ids <- names(cont$word_matrix) # extract the row-indices to get the meta-data
  
  get_story <- function(story_id){ # helper function to query every story-id
    url <- glue(base_url,'single/', story_id, '?key=', mc_key) # Endpoint and API-key
    content(GET(url)) %>%  # Get the Meta-info and format it prettier
      bind_cols() %>% 
      mutate(across(everything(),
                    ~as.character(.)))
  }
  
  v <- lapply(cont$word_matrix, unlist) # count per document as list of respective vectors
  
  r_names <- story_ids # doc-names for our doc-term matrix
  
  c_names <- map_chr(cont$word_list,
                     ~nth(.,2)) # get the first non-stemmed term as the words for the doc-term matrix
  
  if(length(v)>0){ # check if there even is any word, i.e. if any article was collected
    list( # return a list with
      triplet = # a simple triplet that indicates every nun-zero combination of words and documents
        simple_triplet_matrix(
          i = unlist(map(seq_along(v),  ~ rep(., length(
            v[[.]]
          ))), use.names = F),
          j = unlist(map(v,  ~ as.numeric(names(
            .
          ))), use.names = F) + 1,
          v = unlist(v, use.names = F),
          dimnames = list(rows = r_names,
                          cols = c_names)
        ),
      story_info = # and a tibble with all meta-information
        story_ids %>%
        map_dfr( ~ get_story(.))
    )
  }else{
    return(list()) # if there is no word in the matrix return an empty list
  }
}

```

Since we are limiting every query to 2000 stories to reduce network-strain, we will send a query for every week in 2020. We therefore construct a vector of date-ranges.

```{r, eval = F}
weeks <- paste0('[',
                as.Date('2020-01-01') +lubridate::weeks(c(0:51)),
                'T00:00:00Z TO ',
                as.Date('2020-01-01') +lubridate::weeks(c(1:52)),
                'T00:00:00Z]')

```

The last step is to define the actual query-terms. Since we want to see only the reporting about the pandemic in which the respective other country is mentioned, we settled on the boolean combination of terms for corona and terms for the other nationality, i.e. `(covid OR corona) AND (tyskland OR tysk)` for Sweden and `(covid OR corona) AND (schweden OR schwedisch)` for Germany.

Now everything is prepared to run our queries:

```{r, eval = F}

plan(multicore(workers = 8))

swedish_stories <- weeks %>%
  future_map( ~ get_stories(
    .,
    '(covid OR corona) AND (tyskland OR tysk)',
    swe_sources$media_id
  ),.progress = T)

german_stories <- weeks %>%
  future_map( ~ get_stories(
    .,
    '(covid OR corona) AND (schweden OR schwedisch)',
    ger_sources$media_id
  ), .progress = T)
```



```{r, include=F, eval = F}
load('./../data/collected_data.RData')
```


### Combining the results

The first step in processing the data is to remove all empty lists that indicate weeks with no articles whatsoever. This can be achieved with a simple filter:

```{r, eval = F}

german_stories %<>% .[which(map_lgl(.,  ~ length(.) > 1))]
swedish_stories %<>% .[which(map_lgl(.,  ~ length(.) > 1))]

```

We then need to do some processing to end up with the datasets that we need.
Since we used forked processing to gather our data, we have to combine it. The aim is to end up with four objects; one meta-tibble and one document-term matrix for each country.

#### Meta-data

To aggregate our meta-datasets we first need to define a few helper-functions which mostly do a bit of cleaning since the resulting data-frames are a bit in-consistent.

```{r, eval = F}

first_non_na <- function(row){
  first(row[which(!is.na(row))])
} # a simple helper to reduce the strange amount of empty story-id columns

extract_meta <- function(stories){ # a cleaner-function to end up with one streamlined meta-dataset for each country
  stories %>%
    map_dfr( ~ nth(., 2) %>%
               select(publish_date, guid, title, url,media_id, matches('stories_id'))) %>%
    rowwise() %>%
    mutate(stories_id = first_non_na(c_across(matches('stories')))) %>%
    select(-matches('stories_id.+')) %>%
    distinct
}

german_meta <- german_stories %>% 
  extract_meta() %T>%
  write_rds('./../data/german_meta.rds') #save for later

swedish_meta <- swedish_stories %>% 
  extract_meta() %T>%
  write_rds('./../data/swedish_meta.rds')#save for later

```

#### Document-Term-Matrices

To combine the triplet-matrices, we need to define some helpers again. A simple row-binding of the matrices would not do, since every set of articles will have a separate set of words.

```{r, eval = F}

resolve_redundancies <- function(mat1,mat2=NULL){ #combines two triplets into one
  if(is.null(mat2)){
    input <- list(mat1)
  }else{
    input <- list(mat1, mat2)
  }
  word_tibble <- input %>%
    map_dfr(~ tibble(
      story = .$dimnames$rows[.$i],
      word = .$dimnames$cols[.$j],
      count = .$v
    )) %>%
    mutate(i = as.numeric(as.factor(story)),
           j = as.numeric(as.factor(word)))
    return(simple_triplet_matrix(word_tibble$i,
                                 word_tibble$j,
                                 word_tibble$count,
                                 dimnames = list(rows = sort(unique(word_tibble$story)),
                                                 cols = sort(unique(word_tibble$word)))))
  
}

combine_all_triplets <- function(triplets, language){ # iterative wrapper to combine all sets
  require(SnowballC)
  out <-  triplets[[1]]
  for(i in 2:length(triplets)){
    out <- resolve_redundancies(out, triplets[[i]])
  }
  # finally we'll stem it but save the original words
  
  stem_lex <- tibble(long_ver = out$dimnames$cols,
                     stems = wordStem(long_ver, language = language)) %>% 
    nest_by(stems)
  out$dimnames$cols %<>% wordStem(language = language)
  out %<>% resolve_redundancies()
  return(list(dtm = out, stem_lex = stem_lex))
}
```

These helpers can then be used to assemble our final matrices.

```{r, eval = F}
german_doc_term <- german_stories %>% 
  map(~first(.)) %>% 
  combine_all_triplets('german')


write_rds(first(german_doc_term),
            './../data/german_dtm.rds',
            compress = 'gz') #save for later

write_rds(nth(german_doc_term,2),
            './../data/german_stem_keys.rds',
            compress = 'gz')



swedish_doc_term <- swedish_stories %>% 
  map(~first(.)) %>% 
  combine_all_triplets('swedish') 


write_rds(first(swedish_doc_term),
            './../data/swedish_dtm.rds',
            compress = 'gz') #save for later

write_rds(nth(swedish_doc_term,2),
            './../data/swedish_stem_keys.rds',
            compress = 'gz')
```


### Pre-processing for the text visualisations

To end up with some nice and sensible results, we need to do some additional text-cleaning.
The first step is to remove all unnecessary words, since there are a lot of 'filler-words' that are useful for the reader but useless for our visualisations.

As usual, we start by defining a helper-function to do our cleaning. We went with quite a radical solution, removing all words shorter than 4 symbols, all character strings with non-letter contents and a high cut-off for the term frequency - inverse document frequency value, a value indicating how important each word is to individual documents in the corpus. This indicator is the product of the average term-frequency $$tf_t = {1\over N_d}\sum_{i=1}^{N_d} tf_{t, i}$$ where $N_d$ is the total amount of documents in the corpus and  $$tf_{t, d} = {f_{t,d}\over{\sum_{i=1}^{N_t}f_{i, d}}}$$ where $t$ is a given term, $N_t$ is the total number of terms in the corpus and $f_{t,d}$ is the frequency of a given term in a given document with the inverse document frequency $$idf_{t,D} = \log_{10}{ {N_d}\over{|\{d \in D : t \in d\}|}}$$ where  $|\{d \in D : t \in d\}|$ is the number of documents that mention the respective term at least once.
 

```{r, eval = T}

remove_stoppers <- function(dtm, language = 'german'){
  require(tm) # for the stopwords
  require(SnowballC)
  
  log_idx <- str_detect(dtm$dimnames$cols,'[^[:alpha:]]', negate = T) & 
             str_count(dtm$dimnames$cols) > 3 &
             !(dtm$dimnames$cols %in% wordStem(stopwords(language),language = language))
  
  dtm %<>% .[,log_idx]
  
  TF_IDF <-  tapply(dtm$v/row_sums(dtm)[dtm$i], dtm$j, mean) *  log10(dtm$nrow/col_sums(dtm > 0))
  dtm %<>% .[,TF_IDF > 1.75 * median(TF_IDF)] #arbitrary cutoff
  dtm
}
```

This filter-function is then applied to both matrices:

```{r, eval = T}
swedish_dtm <- read_rds('./../data/swedish_dtm.rds') %>% 
  remove_stoppers(language = 'swedish')

german_dtm <- read_rds('./../data/german_dtm.rds') %>% 
  remove_stoppers(language = 'german')
```
## Sentiments {.tabset}

For the third of our visualisations, we now need to determine the sentiment for the terms left in our corpus. 

### Swedish

For the Swedish set, we will use a dictionary provided by github user [AlexGustafsson](https://github.com/AlexGustafsson) which is available [here](https://github.com/AlexGustafsson/sentiment-swedish/blob/develop/build/AFINN-sv-165.txt).

```{r, eval = T}
swedish_word_freq <- swedish_dtm %$%
  tibble(word = dimnames$cols[j],
         n = v) %>% 
  group_by(word) %>% 
  summarise(n = sum(n))

swedish_word_freq %>% 
  skimr::skim()

swe_sent <- read_lines('https://raw.githubusercontent.com/AlexGustafsson/sentiment-swedish/develop/build/AFINN-sv-165.txt')

swe_sent <- tibble(
  word = str_remove(swe_sent, '\\s\\-?\\d$'),
  sentiment = as.numeric(str_extract(swe_sent, '\\-?\\d$'))) %>% 
  mutate(word = wordStem(word, language = 'swedish')) %>% 
  group_by(word) %>% 
  summarise(sentiment = mean(sentiment)) %>% 
  right_join(swedish_word_freq,by = c('word' = 'word')) 

swe_sent %>% 
  skimr::skim()


```

### German
For the German sentiments, we will use the `SentiWS` set [@remusSentiWSAPubliclyAvailable2010], as obtained from [here](https://www.kaggle.com/rtatman/german-sentiment-analysis-toolkit).

```{r, eval = T}
 german_word_freq<- german_dtm %$%
  tibble(word = dimnames$cols[j],
         n = v) %>% 
  group_by(word) %>% 
  summarise(n = sum(n))


ger_sent <- read_tsv('./../data/german_sentis.txt', col_names = F) %>% 
  select(1:2) %>% 
  set_names(c('word', 'sentiment')) %>% 
  mutate(word = str_remove(word, '\\|.+$'),
         word = str_to_lower(word),
         word = wordStem(word, language = 'german')) %>% 
  group_by(word) %>% 
  summarise(sentiment = mean(sentiment)) %>%
  right_join(german_word_freq,by = c('word' = 'word')) 


ger_sent %>% 
  skimr::skim()
```


### Translation

```{r, include= F, eval = F}
save(german_word_freq, swedish_word_freq, file = './../data/word_freqs.RData')
```


Since the non-English dictionaries we found are rather incomplete, we'll try one version with translated words. This does mean though, that we can only look at a limited amount of terms since the translation-API we use is limited.

Let's first define a helper function to use the [MyMemory](https://mymemory.translated.net/doc/spec.php) translation API.

```{r, eval = F}
translate_with_mymemory <- function(word, from, to = 'en'){
  .GlobalEnv$translated <- 0
  require(httr)
  ret <- GET(glue('https://api.mymemory.translated.net/get?q=',
                  word, '&langpair=',
                  from, '|', to,
                  '&de=max.brede@student.fh-kiel.de'))
  out <- tryCatch({
    content(ret)$matches %>%
      map_dfr( ~ bind_cols(.) %>%  mutate(across(everything(), ~ as.character(.)))) %>%
      dplyr::slice(which.max(quality)) %>%
      select(translation, quality) %>%
      transmute(word = word,
                translation,
                quality)
  },
  error = function(e) {
    word
  })
  return(out)
  
}
```


We'll then use this helper to translate the 5000 most used words in both matrices.

```{r, eval = F}
plan(multicore(workers = 8))

german_keys <- read_rds('./../data/german_stem_keys.rds') 

german_word_freq %<>% 
  arrange(desc(n)) %>% 
  slice(1:5000) %>%
  left_join(german_keys, by = c('word' = 'stems')) %>% 
  transmute(word,n,long = map_chr(data,~first(unlist(.)))) %>% 
  mutate(tranlation = future_map(long, ~translate_with_mymemory(.,'de'),.progress = T))

german_word_freq %>% 
  write_rds('./../data/german_translated.rds', compress = 'gz')



swedish_keys <- read_rds('./../data/swedish_stem_keys.rds') 

swedish_word_freq %<>% 
  arrange(desc(n)) %>% 
  slice(1:5000) %>%
  left_join(swedish_keys, by = c('word' = 'stems')) %>% 
  transmute(word,n,long = map_chr(data,~first(unlist(.)))) %>% 
  mutate(tranlation = future_map(long, ~translate_with_mymemory(.,'sv'), .progress = T))

swedish_word_freq %>% 
  write_rds('./../data/swedish_translated.rds', compress = 'gz')
```


Let's now see, how good the API really is (after doing some minor cleaning).

```{r, eval =T}

clean_myMemory <- function(translated_set){
  translated_set %>% 
    transmute(stem = `word`,
              long,
              translation = map_chr(tranlation,~tryCatch(.$translation, error = function(e){.})),
              translation = str_to_lower(translation),
              translation = str_remove_all(translation,'\\W'),
              translation = str_trim(translation),
              n) 
}

ger_tranlated <- read_rds('./../data/german_translated.rds') %>% 
  clean_myMemory() #%T>% 
  #write_csv('data/manually_cleaned_german.csv')
  
DT::datatable(ger_tranlated)

swe_translated <-  read_rds('./../data/swedish_translated.rds')%>% 
  clean_myMemory() #%T>%
  #write_csv('data/manually_cleaned_swedish.csv') 

DT::datatable(swe_translated)
```

Unfortunately, the quality of the translation is quite inconsistent. To tackle that, we conducted a manual correction of the 250 most common terms in Excel.

### Finnishing up

The last step is to

  1. assign sentiment values to the keys
  2. come back to the documents and add the sentiments to our metadata-frame
  
We will try to augment the English approach with the multilingual one but the results will be far from perfect.

So let's first add sentiments using the sentiments as indicated in the [AFINN-set](http://www2.imm.dtu.dk/pubdb/pubs/6010-full.html) [@nielsenNewANEWEvaluation2011], (though it was originally intended for microblogging) and the `sentiments`-df from the `tidytext` package, which is the set as reported by @huMiningSummarizingCustomer2004. Since the sentiments in the latter are only indicated as 'positive' and 'negative', we need to transform this to the endpoints of the AFINN-set (-5,5). This is obviously a heavily skewed approach, but we need to accept imperfections at some point. We will use the higher resolution AFINN-sentiments where we can though.

```{r, eval = T}

add_sentiments <- function(translated_set){
  translated_set%>% 
    left_join(read_tsv('./../data/AFINN-111.txt', col_names = c('term', 'sentiment')),
              by = c('translation' = 'term')) %>% 
    left_join(tidytext::sentiments, 
              by = c('translation' = 'word')) %>% 
    rename(sentiment_AFINN = sentiment.x, sentiment_tidytext = sentiment.y) %>% 
    mutate(sentiment_tidytext = recode(sentiment_tidytext, "positive" = 5, 'negative' = -5),
           sentiment = ifelse(is.na(sentiment_AFINN), sentiment_tidytext, sentiment_AFINN))
}

ger_tranlated <- read_csv('./../data/manually_cleaned_german.csv') %>% 
  add_sentiments()

swe_translated <- read_csv('./../data/manually_cleaned_swedish.csv') %>% 
  add_sentiments()

```

The next step is to now combine these results with the multilingual approach (after scaling the German sentiments from that approach to the -5:5 scale). Since the two sentiment measurements are bound to differ, we'll average them wherever there are two values.

```{r, eval = T, cache=T}
ger_sent %<>% 
  mutate(sentiment = sentiment/max(abs(sentiment),na.rm = T) * 5) %>% 
  left_join(ger_tranlated[,c('stem','translation', 'sentiment')], by = c('word' = 'stem')) %>% 
  rowwise() %>% 
  mutate(sentiment = mean(c_across(matches('sentiment')), na.rm = T)) %>% 
  select(word, translation, sentiment, n)
  

skimr::skim(ungroup(ger_sent))


swe_sent %<>% 
  left_join(swe_translated[,c('stem','translation', 'sentiment')], by = c('word' = 'stem')) %>% 
  rowwise() %>% 
  mutate(sentiment = mean(c_across(matches('sentiment')), na.rm = T)) %>% 
  select(word,translation, sentiment, n)

skimr::skim(ungroup(swe_sent))

```

### Merging and exporting the final sets

For the two visualisations we had in mind we'll need two data sets. 

1. the overall count and sentiment per country

2. the time course for each country, including sentiments and article counts


The first one is easy, we just need to combine the sentiment sets we created in the last step:

```{r, eval = F}
imap_dfr(list('Sweden' = swe_sent,
              'Germany' = ger_sent),
          ~mutate(.x, country = .y)) %>% 
  write_rds('./../data/wordcloud_data.rds',compress = 'gz')
```

The second one is a bit more tricky, since we need to look up in the respective document term matrices, which document used which terms in which frequency to then average the respective sentiments. This sentiment-per-document-set can then be joined with the existing metadata.

```{r, eval = F}
get_sentiment_per_document <- function(dtm, sentiment){
  
  extract_counts <- function(id,dtm, sentiment){
    tibble(n = dtm$v[dtm$i == which(dtm$dimnames$rows == id)],
           term = dtm$dimnames$cols[dtm$j[dtm$i == which(dtm$dimnames$rows == id)]]) %>% 
      left_join(sentiment[,1:2], by = c('term' = 'word')) %>% 
      mutate(partial_sent = n * sentiment/sum(n)) %$% 
      tibble(id = id,
             m_sentiment = sum(sentiment, na.rm = T))
  }
  
  map_dfr(dtm$dimnames$rows, ~extract_counts(., dtm, sentiment))
}


swe_sent_per_doc <- get_sentiment_per_document(swedish_dtm, swe_sent)
ger_sent_per_doc <- get_sentiment_per_document(german_dtm, ger_sent)


ger_part <- ger_sent_per_doc %>% 
  mutate(country = 'Germany') %>% 
  right_join(read_rds('./../data/german_meta.rds'), by=c('id' = 'stories_id')) %>% 
  mutate(media_id = as.numeric(media_id)) %>% 
  left_join(ger_sources[,c('media_id', 'pub_state')]) %>% 
  select(country, title,m_sentiment, publish_date, everything())
  
swe_part <- swe_sent_per_doc %>% 
  mutate(country = 'Sweden') %>% 
  right_join(read_rds('./../data/swedish_meta.rds'), by=c('id' = 'stories_id')) %>% 
  mutate(media_id = as.numeric(media_id)) %>% 
  left_join(swe_sources[,c('media_id', 'pub_state')]) %>% 
  select(country, title,m_sentiment, publish_date, everything())

bind_rows(ger_part, swe_part) %>% 
  write_rds('./../data/documents_with_sentiments.rds', compress = 'gz')



```


## Policy data
For attaining the COVID-19 policy data we read the data from the github of the
[Coronavirus Government Response Tracker Project](https://www.bsg.ox.ac.uk/research/research-projects/coronavirus-government-response-tracker#data).

```{r}
# read policies csv-file
policies <- read.csv('https://raw.githubusercontent.com/OxCGRT/covid-policy-tracker/master/data/OxCGRT_latest.csv')
```

For the meanings of specific policies take a look on the [Codebook](https://github.com/OxCGRT/covid-policy-tracker/blob/master/documentation/codebook.md) of the project.

A function for the data pre-filtering:
```{r}
# a function for pre filtering
filtering <- function(country_name, policy_filter) {
   policies_ger <- policies %>%
                filter(CountryName==country_name) %>% # filter policies by germany
                select(any_of(policy_filter)) %>% # select policies of interest
                mutate(Date=as.Date(as.character(Date),
                                    format = "%Y%m%d")) %>% # change date format
                na.omit() %>% # remove rows with na entries
                arrange(Date) # order rows by date
}
```


The data has to be restructured to ease the plotting. So that's the
function for doing that:
```{r}
# a function to build start and stop labels for the equivalent policy
build_start_stop_labels <- function(policies, policy, label){
  # get selected policy data
  selected_policy <- policies %>%
                      select(Date, policy)
  # build bit-mask for policy changes
  bit_mask <- selected_policy[,policy] -  lag(selected_policy[,policy], n = 1) != 0
  # set last entry true (is na because of lag)
  bit_mask[is.na(bit_mask)] <- TRUE
  
  # initialize start and stop list
  start <- c()
  stop <- c()
  # initialize a state variable for tracking start position
  start_state <- TRUE
  # iterate through rows for finding start and stop positions
  for(row in 1:length(bit_mask)){
    # check whether it's a stop position
    if(bit_mask[row] & !start_state){
      stop <- c(stop, row-1)
      start_state <- TRUE
    }
    # check whether it's a start position
    if(bit_mask[row] & start_state){
      start <- c(start, row)
      start_state <- FALSE
    }
  }
  # add last row as stop
  stop <- c(stop, row)
  
  # retrieve start and stop dates from the data set
  start_date <- selected_policy[start, 'Date']
  stop_date <- selected_policy[stop, 'Date']
  # retrieve event number from the data set
  events <- selected_policy[start, policy]
  
  # return dataframe with start/stop dates for the polcies
  return(data.frame(event = events, start = start_date, end = stop_date,
                    group = label))
}
```

After that we build the time line data for the restrictions on gatherings. And save it for plotting later.
```{r}
# set filter for columns of interest
policy_filter <- c('CountryCode', 'Date','C4_Restrictions.on.gatherings')
# retrieve policy data for germany and sweden
policies_ger <- filtering('Germany', policy_filter)
policies_se <- filtering('Sweden', policy_filter)
# rebuild data into start and stop labels
timeline_data <- rbind(build_start_stop_labels(policies_ger,
                                               'C4_Restrictions.on.gatherings',
                                               'Germany'),
                       build_start_stop_labels(policies_se,
                                               'C4_Restrictions.on.gatherings',
                                               'Sweden'))
# save the data as an csv-file
write.csv(timeline_data, './../data/timeline_data.csv')
```

## Incidence data

Finally, we'll collect the incidences over time to compare to our other variables:

```{r collect covid, eval = F}
# retrieve covid data from covid.ourworldindata.org git repository
timeseries_data <- read.csv('https://covid.ourworldindata.org/data/owid-covid-data.csv')
# filter for Germany and Sweden data and select location, date and smoothed new
# cases per million
timeseries_data %<>% filter(location=='Sweden' | location=='Germany') %>%
          select(location, date,
                 new_cases_smoothed_per_million) %>% # select columns of interest
          na.omit() %>% # remove rows with na entries             
          mutate(date=as.Date(as.character(date),
                              format = "%Y-%m-%d"))  # save date in date format
# save the data as an csv-file
write.csv(timeseries_data, './../data/timeseries_data.csv')
```

<!-- ## References -->
