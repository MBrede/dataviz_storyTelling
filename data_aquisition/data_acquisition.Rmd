---
title: "acquisition"
output:
  # pdf_document:
  #   includes:
  #     in_header: header_doc.tex
  html_document:
    css: styles.css
    fig_caption: true
    toc: true
csl: apa.csl
bibliography: references.bib
---

```{r setup, include=FALSE}
library(knitr) # import of stuff
library(kableExtra) # nice tables
library(tidyverse) # general handling

library(glue) # for text-string handling
library(slam) # for memory efficient matrix-representations
library(magrittr) #for additional pipes


opts_chunk$set(echo = T,message = F, fig.align = 'center', eval = F)

theme_set(theme_light())

```

# Quellen für News-Artikel

We want to look at the German news reports about the Swedish Corona-response and vice versa.

To do this, we will use the after a free registration openly available [Mediacloud](https://mediacloud.org/#/Home) API to gather news-articles for the time frame since January of 2020.

The API allows users to search for terms for a given set of news-media outlets in addition to some filter settings like a given time-frame and such. These queries can be sent to a whole range of API endpoints, including one that returns document term matrices and one that returns all gathered meta-information MediaCloud collected.

Since we only want to look at german and swedish media outlets, we will be using two "Collections" of outlets as provided by Mediacloud. The selections we will be using are discussed in the following section.

 
## Sources {.tabset}

### Sweden
We will use a collection of [Swedish](https://sources.mediacloud.org/#/collections/34412223) news-outlets with focus on local and national news as provided by Mediacloud. 

Since we do not have the necessary domain-knowledge to judge the validity of this selection, we will use it unchanged except for a filter that removes any source who's main language is not swedish. The resulting selection consists of the outlets listed in table 1.

```{r, eval = T}
swe_sources <- read_csv('data/sweden.csv') %>% # Sources of Swedish news
  filter(language == 'sv') %>% 
  select(media_id, url, name, stories_per_day, pub_state)

swe_sources %>% 
  select(title = name,
         URL = url,
         `average number of articles per day` = stories_per_day) %>% 
  arrange(desc(`average number of articles per day`)) %>% 
  DT::datatable(caption = 'Table 1: the selection of Swedish news-outlets used in our analysis.')

```



### Germany

We will use a collection of [German](https://sources.mediacloud.org/#/collections/38379816) news-outlets with focus on local and national news as provided by Mediacloud. 

After a superficial check of the sets validity, the presence of Russian outlets became aparent. But since these were correctly marked as being in Russian, they could be easily removed by filtering for German sources only. 

We additionaly realized, that the Spiegel as one of Germanys largest outlets is missing from the collection, so we added it after searching the Mediacloud-Page for it's ID.

```{r, eval = T}
ger_sources <- read_csv('data/germany.csv') %>% # Sources of German News
  filter(language == 'de') %>%
  select(media_id, url, name, stories_per_day, pub_state) %>%
  bind_rows(tibble(media_id = 19831,
                   url = 'https://www.spiegel.de',
                   name = 'der Spiegel',
                   stories_per_day = 82,
                   pub_state = NA))
  
ger_sources %>% 
  select(title = name,
         URL = url,
         `average number of articles per day` = stories_per_day) %>% 
  arrange(desc(`average number of articles per day`)) %>% 
  DT::datatable(caption = 'Table 2: the selection of German news-outlets used in our analysis.')
```




```{r, include=F}
save(list = ls(),file = 'data/pre_steps.RData')
```


## data collection

We then use the collections to query the API for the matrices and the story-meta-information. To do this, we first build our query-function using httr:

```{r}

get_stories <- function(time_range,query,media_ids){
  require(httr) # GET and POST-requests
  
  base_url <- 'https://api.mediacloud.org/api/v2/stories_public/' # basic API-URL
  
  query_string <- glue('?q=text:', query) # add the query
  
  url <- glue(base_url,'word_matrix', query_string, '&key=', mc_key) # add the endpoint and the API-key
  
  resp <-
    POST(
      glue(
        url,
        '&publish_day=',time_range, # add time range
        '&rows=2000&stopword_length=long' # add the short filters
      ),
      body = jsonlite::toJSON(list(fq = list(media_id = media_ids))) # add the longer filters
    ) # POST the request
  
  cont <- content(resp) # get the response
  
  story_ids <- names(cont$word_matrix) # extract the row-indices to get the meta-data
  
  get_story <- function(story_id){ # helper function to query every story-id
    url <- glue(base_url,'single/', story_id, '?key=', mc_key) # Endpoint and API-key
    content(GET(url)) %>%  # Get the Meta-info and format it prettier
      bind_cols() %>% 
      mutate(across(everything(),
                    ~as.character(.)))
  }
  
  v <- lapply(cont$word_matrix, unlist) # count per document as list of respective vectors
  
  r_names <- story_ids # doc-names for our doc-term matrix
  
  c_names <- map_chr(cont$word_list,
                     ~nth(.,2)) # get the first non-stemmed term as the words for the doc-term matrix
  
  if(length(v)>0){ # check if there even is any word, i.e. if any article was collected
    list( # return a list with
      triplet = # a simple triplet that indicates every nun-zero combination of words and documents
        simple_triplet_matrix(
          i = unlist(map(seq_along(v),  ~ rep(., length(
            v[[.]]
          ))), use.names = F),
          j = unlist(map(v,  ~ as.numeric(names(
            .
          ))), use.names = F) + 1,
          v = unlist(v, use.names = F),
          dimnames = list(rows = r_names,
                          cols = c_names)
        ),
      story_info = # and a tibble with all meta-information
        story_ids %>%
        map_dfr( ~ get_story(.))
    )
  }else{
    return(list()) # if there is no word in the matrix return an empty list
  }
}

```

Since we are limiting every query to 2000 storys to reduce network-strain, we will send a query for every week in 2020. We therefore construct a vector of date-ranges.

```{r}
weeks <- paste0('[',
                as.Date('2020-01-01') +lubridate::weeks(c(0:51)),
                'T00:00:00Z TO ',
                as.Date('2020-01-01') +lubridate::weeks(c(1:52)),
                'T00:00:00Z]')

```

The last step is to define the actual query-terms. Since we want to see only the reportings about corona in which the respective other country is mentioned, we settled on the boolean combination of terms for corona and terms for the other nationality, i.e. `(covid OR corona) AND (tyskland OR tysk)` for Sweden and `(covid OR corona) AND (schweden OR schwedisch)` for Germany.

Now everything is prepared to run our queries. Since we wanted it to run quickly, we cheated a bit and ran it in a separate script, but we used the following code:

```{r}
library(furrr) # parallel-processing

plan(multicore(workers = 8))

swedish_stories <- weeks %>%
  future_map( ~ get_stories(
    .,
    '(covid OR corona) AND (tyskland OR tysk)',
    swe_sources$media_id
  ),.progress = T)

german_stories <- weeks %>%
  future_map( ~ get_stories(
    .,
    '(covid OR corona) AND (schweden OR schwedisch)',
    ger_sources$media_id
  ), .progress = T)
```



```{r, include=F}
load('data/collected_data.RData')
```

## the "reduce"-part {.tabset}

The first step in processing the data is to remove all empty lists that indicate weeks with no articles whatsoever. This can be achieved with a simple filter:

```{r}

german_stories %<>% .[which(map_lgl(.,  ~ length(.) > 1))]
swedish_stories %<>% .[which(map_lgl(.,  ~ length(.) > 1))]

```

We then need to do some processing to end up with the datasets that we need.
Since we used forked processing to gather our data, we have to combine it. The aim is to end up with four objects; one meta-tibble and one document-term matrix for each country.

### meta-data

To aggregate our our meta-datasets we first need to define a few helper-functions which mostly do a bit of cleaning since the resulting data-frames are a bit in-consistent.

```{r}

first_non_na <- function(row){
  first(row[which(!is.na(row))])
} # a simple helper to reduce the strange amount of empty story-id columns

extract_meta <- function(stories){ # a cleaner-function to end up with one streamlined meta-dataset for each country
  stories %>%
    map_dfr( ~ nth(., 2) %>%
               select(publish_date, guid, title, url, matches('stories_id'))) %>%
    rowwise() %>%
    mutate(stories_id = first_non_na(c_across(matches('stories')))) %>%
    select(-matches('stories_id.+')) %>%
    distinct
}

german_meta <- german_stories %>% 
  extract_meta() %T>%
  write_rds('data/german_meta.rds') #save for later

swedish_meta <- swedish_stories %>% 
  extract_meta() %T>%
  write_rds('data/swedish_meta.rds')#save for later

```

### Document-Term-Matrices

To combine the triplet-matrices, we need to define some helpers again. A simple row-binding of the matrices would not do, since every set of articles will have a seperate set of words. So we will need to use some R-trickery.

```{r}

resolve_redundancies <- function(mat1,mat2=NULL){ #combines two triplets into one
  if(is.null(mat2)){
    input <- list(mat1)
  }else{
    input <- list(mat1, mat2)
  }
  word_tibble <- input %>%
    map_dfr(~ tibble(
      story = .$dimnames$rows[.$i],
      word = .$dimnames$cols[.$j],
      count = .$v
    )) %>%
    mutate(i = as.numeric(as.factor(story)),
           j = as.numeric(as.factor(word)))
    return(simple_triplet_matrix(word_tibble$i,
                                 word_tibble$j,
                                 word_tibble$count,
                                 dimnames = list(rows = sort(unique(word_tibble$story)),
                                                 cols = sort(unique(word_tibble$word)))))
  
}

combine_all_triplets <- function(triplets, language){ # iterative wrapper to combine all sets
  require(SnowballC)
  out <-  triplets[[1]]
  for(i in 2:length(triplets)){
    out <- resolve_dedundancies(out, triplets[[i]])
  }
  # finally we'll stem it but save the original words
  
  stem_lex <- tibble(long_ver = out$dimnames$cols,
                     stems = wordStem(long_ver, language = language)) %>% 
    nest_by(stems)
  out$dimnames$cols %<>% wordStem(language = language)
  out %<>% resolve_redundancies()
  return(list(dtm = out, stem_lex = stem_lex))
}
```

These helpers can then be used to assemble our final matrices.

```{r}
german_doc_term <- german_stories %>% 
  map(~first(.)) %>% 
  combine_all_triplets('german')


write_rds(first(german_doc_term),
            'data/german_dtm.rds',
            compress = 'gz') #save for later

write_rds(nth(german_doc_term,2),
            'data/german_stem_keys.rds',
            compress = 'gz')



swedish_doc_term <- swedish_stories %>% 
  map(~first(.)) %>% 
  combine_all_triplets('swedish') 


write_rds(first(swedish_doc_term),
            'data/swedish_dtm.rds',
            compress = 'gz') #save for later

write_rds(nth(swedish_doc_term,2),
            'data/swedish_stem_keys.rds',
            compress = 'gz')
```


## pre-processing for the text visualisations

To end up with some nice and sensible results, we need to do some additional text-cleaning.
The first step is to remove all unnecessary words, since there are a lot of in our context 'filler-words that are usefull for the reader but useless for our visualisations.

As usual, we start by defining a helper-function to do our cleaning. We went with quite a radical solution, removing all words shorter than 4 symbols, all character-strings with non-letter contents and a high cutoff for the term frequency - inverse document frequency value, a value indicating how important each word is to individual documents in the corpus. This indicator is the product of the average term-frequency $$tf_t = {1\over N_d}\sum_{d=1}^{N_d} tf_{t, d}$$ where $N_d$ is the total amount of documents in the corpus and  $$tf_{t, d} = {f_{t,d}\over{\sum_{i=1}^{N_t}f_{i, d}}}$$ where $t$ is a given term, $N_t$ is the total number of terms in the corpus and $f_{t,d}$ is the frequency of a given term in a given document with the inverse document frequency $$idf_{t,D} = \log{ {N_d}\over{|\{d \in D : t \in d\}|}}$$ where  $|\{d \in D : t \in d\}|$ is the number of documents that mention the respective term at least once.
 

```{r, eval = T}

remove_stoppers <- function(dtm, language = 'german'){
  require(tm) # for the stopwords
  require(SnowballC)
  
  log_idx <- str_detect(dtm$dimnames$cols,'[^[:alpha:]]', negate = T) & 
             str_count(dtm$dimnames$cols) > 3 &
             !(dtm$dimnames$cols %in% wordStem(stopwords(language),language = language))
  
  dtm %<>% .[,log_idx]
  
  TF_IDF <-  tapply(dtm$v/row_sums(dtm)[dtm$i], dtm$j, mean) *  log(dtm$nrow/col_sums(dtm > 0))
  dtm %<>% .[,TF_IDF > 1.75 * median(TF_IDF)] #arbitrary cutoff
  dtm
}
```

This filter-function is then applied to both matrices:

```{r, eval = T}
swedish_dtm <- read_rds('data/swedish_dtm.rds') %>% 
  remove_stoppers(language = 'swedish')

german_dtm <- read_rds('data/german_dtm.rds') %>% 
  remove_stoppers(language = 'german')
```
## sentiments{.tabset}

For the second of our visualisations we now need to  determine the sentiment for the terms left in our corpus. 

### Swedish

For the Swedish set, we will use a dicionary provided by github user [AlexGustafsson](https://github.com/AlexGustafsson which is available [here](https://github.com/AlexGustafsson/sentiment-swedish/blob/develop/build/AFINN-sv-165.txt).

```{r, eval = T}
swedish_word_freq <- swedish_dtm %$%
  tibble(word = dimnames$cols[j],
         n = v) %>% 
  group_by(word) %>% 
  summarise(n = sum(n))

swe_sent <- read_lines('https://raw.githubusercontent.com/AlexGustafsson/sentiment-swedish/develop/build/AFINN-sv-165.txt')

swe_sent <- tibble(
  word = str_remove(swe_sent, '\\s\\-?\\d$'),
  sentiment = as.numeric(str_extract(swe_sent, '\\-?\\d$'))) %>% 
  mutate(sentiment = wordStem(word, language = 'swedish')) %>% 
  right_join(swedish_word_freq,by = c('word' = 'word')) 

swe_sent %>% 
  skimr::skim()


```

### German
For the German sentiments, we will use the SentiWS set [@remusSentiWSAPubliclyAvailable2010], as obtained from [here](https://www.kaggle.com/rtatman/german-sentiment-analysis-toolkit).

```{r, eval = T}
 german_word_freq<- german_dtm %$%
  tibble(word = dimnames$cols[j],
         n = v) %>% 
  group_by(word) %>% 
  summarise(n = sum(n))


ger_sent <- read_tsv('data/german_sentis.txt', col_names = F) %>% 
  select(1:2) %>% 
  set_names(c('word', 'sentiment')) %>% 
  mutate(word = str_remove(word, '\\|.+$'),
         word = str_to_lower(word),
         word = wordStem(word, language = 'german')) %>% 
  right_join(german_word_freq,by = c('word' = 'word')) 


ger_sent %>% 
  skimr::skim()
```


## Übersetzen??

```{r}

translate_with_mymemory <- function(word, from, to = 'en'){
  ret <- GET(glue('https://api.mymemory.translated.net/get?q=',
                  word, '&langpair=',
                  from, '|', to,
                  '&de=max.brede@student.fh-kiel.de'))
  httr::content(ret)$matches %>% 
    map_dfr(~bind_cols(.) %>%  mutate(across(everything(), ~as.character(.)))) %>% 
    dplyr::slice(which.max(quality)) %>% 
    select(translation, quality) %>% 
    transmute(word = word, 
              translation,
              quality)
}
```



## References
