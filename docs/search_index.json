[["index.html", "Comparison of Narratives between German and Swedish News Articles during the COVID-19 Pandemic Introduction", " Comparison of Narratives between German and Swedish News Articles during the COVID-19 Pandemic Max Brede und Janik Dunker 2020-12-30 Introduction This document is created using bookdown and presents the results of our DataViz-Assignment in the context of our Data Science studies at the Fachhochschule Kiel. This document is licensed under the CC BY-SA 4.0-license. "],["data-acquisition.html", "Data Acquisition Sources for News-Articles Data Collection Combining the results Pre-processing for the text visualisations Sentiments Translation Merging and exporting the final sets", " Data Acquisition Sources for News-Articles We want to look at the German news reports about the Swedish Corona-response and vice versa. To do this, we will use the after a free registration openly available Mediacloud API to gather news-articles for the time frame since January of 2020. The API allows users to search for terms for a given set of news-media outlets in addition to some filter settings like a given time-frame and such. These queries can be sent to a whole range of API endpoints, including one that returns document term matrices and one that returns all gathered meta-information MediaCloud collected. Since we only want to look at german and swedish media outlets, we will be using two “Collections” of outlets as provided by Mediacloud. The selections we will be using are discussed in the following section. Sweden We will use a collection of Swedish news-outlets with focus on local and national news as provided by Mediacloud. Since we do not have the necessary domain-knowledge to judge the validity of this selection, we will use it unchanged except for a filter that removes any source who’s main language is not swedish. The resulting selection consists of the outlets listed in table 1. swe_sources &lt;- read_csv(&#39;./../data/sweden.csv&#39;) %&gt;% # Sources of Swedish news filter(language == &#39;sv&#39;) %&gt;% select(media_id, url, name,stories_per_day, pub_state) swe_sources %&gt;% select(title = name, URL = url, `average number of articles per day` = stories_per_day) %&gt;% arrange(desc(`average number of articles per day`)) %&gt;% DT::datatable(caption = &#39;Table 1: the selection of Swedish news-outlets used in our analysis.&#39;) Germany We will use a collection of German news-outlets with focus on local and national news as provided by Mediacloud. After a superficial check of the sets validity, the presence of Russian outlets became aparent. But since these were correctly marked as being in Russian, they could be easily removed by filtering for German sources only. We additionaly realized, that the Spiegel as one of Germanys largest outlets is missing from the collection, so we added it after searching the Mediacloud-Page for it’s ID. ger_sources &lt;- read_csv(&#39;./../data/germany.csv&#39;) %&gt;% # Sources of German News filter(language == &#39;de&#39;) %&gt;% select(media_id, url, name, stories_per_day, pub_state) %&gt;% bind_rows(tibble(media_id = 19831, url = &#39;https://www.spiegel.de&#39;, name = &#39;der Spiegel&#39;, stories_per_day = 82, pub_state = NA)) ger_sources %&gt;% select(title = name, URL = url, `average number of articles per day` = stories_per_day) %&gt;% arrange(desc(`average number of articles per day`)) %&gt;% DT::datatable(caption = &#39;Table 2: the selection of German news-outlets used in our analysis.&#39;) Data Collection We then use the collections to query the API for the matrices and the story-meta-information. To do this, we first build our query-function using httr: get_stories &lt;- function(time_range,query,media_ids){ require(httr) # GET and POST-requests base_url &lt;- &#39;https://api.mediacloud.org/api/v2/stories_public/&#39; # basic API-URL query_string &lt;- glue(&#39;?q=text:&#39;, query) # add the query url &lt;- glue(base_url,&#39;word_matrix&#39;, query_string, &#39;&amp;key=&#39;, mc_key) # add the endpoint and the API-key resp &lt;- POST( glue( url, &#39;&amp;publish_day=&#39;,time_range, # add time range &#39;&amp;rows=2000&amp;stopword_length=long&#39; # add the short filters ), body = jsonlite::toJSON(list(fq = list(media_id = media_ids))) # add the longer filters ) # POST the request cont &lt;- content(resp) # get the response story_ids &lt;- names(cont$word_matrix) # extract the row-indices to get the meta-data get_story &lt;- function(story_id){ # helper function to query every story-id url &lt;- glue(base_url,&#39;single/&#39;, story_id, &#39;?key=&#39;, mc_key) # Endpoint and API-key content(GET(url)) %&gt;% # Get the Meta-info and format it prettier bind_cols() %&gt;% mutate(across(everything(), ~as.character(.))) } v &lt;- lapply(cont$word_matrix, unlist) # count per document as list of respective vectors r_names &lt;- story_ids # doc-names for our doc-term matrix c_names &lt;- map_chr(cont$word_list, ~nth(.,2)) # get the first non-stemmed term as the words for the doc-term matrix if(length(v)&gt;0){ # check if there even is any word, i.e. if any article was collected list( # return a list with triplet = # a simple triplet that indicates every nun-zero combination of words and documents simple_triplet_matrix( i = unlist(map(seq_along(v), ~ rep(., length( v[[.]] ))), use.names = F), j = unlist(map(v, ~ as.numeric(names( . ))), use.names = F) + 1, v = unlist(v, use.names = F), dimnames = list(rows = r_names, cols = c_names) ), story_info = # and a tibble with all meta-information story_ids %&gt;% map_dfr( ~ get_story(.)) ) }else{ return(list()) # if there is no word in the matrix return an empty list } } Since we are limiting every query to 2000 storys to reduce network-strain, we will send a query for every week in 2020. We therefore construct a vector of date-ranges. weeks &lt;- paste0(&#39;[&#39;, as.Date(&#39;2020-01-01&#39;) +lubridate::weeks(c(0:51)), &#39;T00:00:00Z TO &#39;, as.Date(&#39;2020-01-01&#39;) +lubridate::weeks(c(1:52)), &#39;T00:00:00Z]&#39;) The last step is to define the actual query-terms. Since we want to see only the reportings about corona in which the respective other country is mentioned, we settled on the boolean combination of terms for corona and terms for the other nationality, i.e. (covid OR corona) AND (tyskland OR tysk) for Sweden and (covid OR corona) AND (schweden OR schwedisch) for Germany. Now everything is prepared to run our queries. Since we wanted it to run quickly, we cheated a bit and ran it in a separate script, but we used the following code: plan(multicore(workers = 8)) swedish_stories &lt;- weeks %&gt;% future_map( ~ get_stories( ., &#39;(covid OR corona) AND (tyskland OR tysk)&#39;, swe_sources$media_id ),.progress = T) german_stories &lt;- weeks %&gt;% future_map( ~ get_stories( ., &#39;(covid OR corona) AND (schweden OR schwedisch)&#39;, ger_sources$media_id ), .progress = T) Combining the results The first step in processing the data is to remove all empty lists that indicate weeks with no articles whatsoever. This can be achieved with a simple filter: german_stories %&lt;&gt;% .[which(map_lgl(., ~ length(.) &gt; 1))] swedish_stories %&lt;&gt;% .[which(map_lgl(., ~ length(.) &gt; 1))] We then need to do some processing to end up with the datasets that we need. Since we used forked processing to gather our data, we have to combine it. The aim is to end up with four objects; one meta-tibble and one document-term matrix for each country. Meta-data To aggregate our our meta-datasets we first need to define a few helper-functions which mostly do a bit of cleaning since the resulting data-frames are a bit in-consistent. first_non_na &lt;- function(row){ first(row[which(!is.na(row))]) } # a simple helper to reduce the strange amount of empty story-id columns extract_meta &lt;- function(stories){ # a cleaner-function to end up with one streamlined meta-dataset for each country stories %&gt;% map_dfr( ~ nth(., 2) %&gt;% select(publish_date, guid, title, url,media_id, matches(&#39;stories_id&#39;))) %&gt;% rowwise() %&gt;% mutate(stories_id = first_non_na(c_across(matches(&#39;stories&#39;)))) %&gt;% select(-matches(&#39;stories_id.+&#39;)) %&gt;% distinct } german_meta &lt;- german_stories %&gt;% extract_meta() %T&gt;% write_rds(&#39;./../data/german_meta.rds&#39;) #save for later swedish_meta &lt;- swedish_stories %&gt;% extract_meta() %T&gt;% write_rds(&#39;./../data/swedish_meta.rds&#39;)#save for later Document-Term-Matrices To combine the triplet-matrices, we need to define some helpers again. A simple row-binding of the matrices would not do, since every set of articles will have a seperate set of words. So we will need to use some R-trickery. resolve_redundancies &lt;- function(mat1,mat2=NULL){ #combines two triplets into one if(is.null(mat2)){ input &lt;- list(mat1) }else{ input &lt;- list(mat1, mat2) } word_tibble &lt;- input %&gt;% map_dfr(~ tibble( story = .$dimnames$rows[.$i], word = .$dimnames$cols[.$j], count = .$v )) %&gt;% mutate(i = as.numeric(as.factor(story)), j = as.numeric(as.factor(word))) return(simple_triplet_matrix(word_tibble$i, word_tibble$j, word_tibble$count, dimnames = list(rows = sort(unique(word_tibble$story)), cols = sort(unique(word_tibble$word))))) } combine_all_triplets &lt;- function(triplets, language){ # iterative wrapper to combine all sets require(SnowballC) out &lt;- triplets[[1]] for(i in 2:length(triplets)){ out &lt;- resolve_redundancies(out, triplets[[i]]) } # finally we&#39;ll stem it but save the original words stem_lex &lt;- tibble(long_ver = out$dimnames$cols, stems = wordStem(long_ver, language = language)) %&gt;% nest_by(stems) out$dimnames$cols %&lt;&gt;% wordStem(language = language) out %&lt;&gt;% resolve_redundancies() return(list(dtm = out, stem_lex = stem_lex)) } These helpers can then be used to assemble our final matrices. german_doc_term &lt;- german_stories %&gt;% map(~first(.)) %&gt;% combine_all_triplets(&#39;german&#39;) write_rds(first(german_doc_term), &#39;./../data/german_dtm.rds&#39;, compress = &#39;gz&#39;) #save for later write_rds(nth(german_doc_term,2), &#39;./../data/german_stem_keys.rds&#39;, compress = &#39;gz&#39;) swedish_doc_term &lt;- swedish_stories %&gt;% map(~first(.)) %&gt;% combine_all_triplets(&#39;swedish&#39;) write_rds(first(swedish_doc_term), &#39;./../data/swedish_dtm.rds&#39;, compress = &#39;gz&#39;) #save for later write_rds(nth(swedish_doc_term,2), &#39;./../data/swedish_stem_keys.rds&#39;, compress = &#39;gz&#39;) Pre-processing for the text visualisations To end up with some nice and sensible results, we need to do some additional text-cleaning. The first step is to remove all unnecessary words, since there are a lot of in our context ’filler-words that are usefull for the reader but useless for our visualisations. As usual, we start by defining a helper-function to do our cleaning. We went with quite a radical solution, removing all words shorter than 4 symbols, all character-strings with non-letter contents and a high cutoff for the term frequency - inverse document frequency value, a value indicating how important each word is to individual documents in the corpus. This indicator is the product of the average term-frequency \\[tf_t = {1\\over N_d}\\sum_{d=1}^{N_d} tf_{t, d}\\] where \\(N_d\\) is the total amount of documents in the corpus and \\[tf_{t, d} = {f_{t,d}\\over{\\sum_{i=1}^{N_t}f_{i, d}}}\\] where \\(t\\) is a given term, \\(N_t\\) is the total number of terms in the corpus and \\(f_{t,d}\\) is the frequency of a given term in a given document with the inverse document frequency \\[idf_{t,D} = \\log{ {N_d}\\over{|\\{d \\in D : t \\in d\\}|}}\\] where \\(|\\{d \\in D : t \\in d\\}|\\) is the number of documents that mention the respective term at least once. remove_stoppers &lt;- function(dtm, language = &#39;german&#39;){ require(tm) # for the stopwords require(SnowballC) log_idx &lt;- str_detect(dtm$dimnames$cols,&#39;[^[:alpha:]]&#39;, negate = T) &amp; str_count(dtm$dimnames$cols) &gt; 3 &amp; !(dtm$dimnames$cols %in% wordStem(stopwords(language),language = language)) dtm %&lt;&gt;% .[,log_idx] TF_IDF &lt;- tapply(dtm$v/row_sums(dtm)[dtm$i], dtm$j, mean) * log(dtm$nrow/col_sums(dtm &gt; 0)) dtm %&lt;&gt;% .[,TF_IDF &gt; 1.75 * median(TF_IDF)] #arbitrary cutoff dtm } This filter-function is then applied to both matrices: swedish_dtm &lt;- read_rds(&#39;./../data/swedish_dtm.rds&#39;) %&gt;% remove_stoppers(language = &#39;swedish&#39;) german_dtm &lt;- read_rds(&#39;./../data/german_dtm.rds&#39;) %&gt;% remove_stoppers(language = &#39;german&#39;) Sentiments For the second of our visualisations we now need to determine the sentiment for the terms left in our corpus. Swedish For the Swedish set, we will use a dictionary provided by github user AlexGustafsson which is available here. swedish_word_freq &lt;- swedish_dtm %$% tibble(word = dimnames$cols[j], n = v) %&gt;% group_by(word) %&gt;% summarise(n = sum(n)) swedish_word_freq %&gt;% skimr::skim() Table 1: Data summary Name Piped data Number of rows 85174 Number of columns 2 _______________________ Column type frequency: character 1 numeric 1 ________________________ Group variables None Variable type: character skim_variable n_missing complete_rate min max empty n_unique whitespace word 0 1 4 34 0 85174 0 Variable type: numeric skim_variable n_missing complete_rate mean sd p0 p25 p50 p75 p100 hist n 0 1 35.47 206.63 1 2 4 12 15223 ▇▁▁▁▁ swe_sent &lt;- read_lines(&#39;https://raw.githubusercontent.com/AlexGustafsson/sentiment-swedish/develop/build/AFINN-sv-165.txt&#39;) swe_sent &lt;- tibble( word = str_remove(swe_sent, &#39;\\\\s\\\\-?\\\\d$&#39;), sentiment = as.numeric(str_extract(swe_sent, &#39;\\\\-?\\\\d$&#39;))) %&gt;% mutate(word = wordStem(word, language = &#39;swedish&#39;)) %&gt;% group_by(word) %&gt;% summarise(sentiment = mean(sentiment)) %&gt;% right_join(swedish_word_freq,by = c(&#39;word&#39; = &#39;word&#39;)) swe_sent %&gt;% skimr::skim() Table 1: Data summary Name Piped data Number of rows 85174 Number of columns 3 _______________________ Column type frequency: character 1 numeric 2 ________________________ Group variables None Variable type: character skim_variable n_missing complete_rate min max empty n_unique whitespace word 0 1 4 34 0 85174 0 Variable type: numeric skim_variable n_missing complete_rate mean sd p0 p25 p50 p75 p100 hist sentiment 83960 0.01 -0.49 2.00 -5 -2 -1.37 2 5 ▁▇▂▃▁ n 0 1.00 35.47 206.63 1 2 4.00 12 15223 ▇▁▁▁▁ German For the German sentiments, we will use the SentiWS set (Remus et al., 2010), as obtained from here. german_word_freq&lt;- german_dtm %$% tibble(word = dimnames$cols[j], n = v) %&gt;% group_by(word) %&gt;% summarise(n = sum(n)) ger_sent &lt;- read_tsv(&#39;./../data/german_sentis.txt&#39;, col_names = F) %&gt;% select(1:2) %&gt;% set_names(c(&#39;word&#39;, &#39;sentiment&#39;)) %&gt;% mutate(word = str_remove(word, &#39;\\\\|.+$&#39;), word = str_to_lower(word), word = wordStem(word, language = &#39;german&#39;)) %&gt;% group_by(word) %&gt;% summarise(sentiment = mean(sentiment)) %&gt;% right_join(german_word_freq,by = c(&#39;word&#39; = &#39;word&#39;)) ## Warning: 807 parsing failures. ## row col expected actual file ## 7 -- 3 columns 2 columns &#39;./../data/german_sentis.txt&#39; ## 13 -- 3 columns 2 columns &#39;./../data/german_sentis.txt&#39; ## 19 -- 3 columns 2 columns &#39;./../data/german_sentis.txt&#39; ## 27 -- 3 columns 2 columns &#39;./../data/german_sentis.txt&#39; ## 33 -- 3 columns 2 columns &#39;./../data/german_sentis.txt&#39; ## ... ... ......... ......... ............................. ## See problems(...) for more details. ger_sent %&gt;% skimr::skim() Table 2: Data summary Name Piped data Number of rows 71303 Number of columns 3 _______________________ Column type frequency: character 1 numeric 2 ________________________ Group variables None Variable type: character skim_variable n_missing complete_rate min max empty n_unique whitespace word 0 1 4 41 0 71303 0 Variable type: numeric skim_variable n_missing complete_rate mean sd p0 p25 p50 p75 p100 hist sentiment 70992 0 -0.02 0.14 -0.61 0 0 0 0.46 ▁▁▇▂▁ n 0 1 14.09 117.01 1.00 1 2 6 14074.00 ▇▁▁▁▁ Translation Since the non-english dictionaries we found are rather uncomplete, we’ll try one version with translated words. This does mean though, that we can only look at a limited amout of terms since the translation-API we use is limited. Let’s first define a helper function to use the MyMemory translation API. translate_with_mymemory &lt;- function(word, from, to = &#39;en&#39;){ .GlobalEnv$translated &lt;- 0 require(httr) ret &lt;- GET(glue(&#39;https://api.mymemory.translated.net/get?q=&#39;, word, &#39;&amp;langpair=&#39;, from, &#39;|&#39;, to, &#39;&amp;de=max.brede@student.fh-kiel.de&#39;)) out &lt;- tryCatch({ content(ret)$matches %&gt;% map_dfr( ~ bind_cols(.) %&gt;% mutate(across(everything(), ~ as.character(.)))) %&gt;% dplyr::slice(which.max(quality)) %&gt;% select(translation, quality) %&gt;% transmute(word = word, translation, quality) }, error = function(e) { word }) return(out) } We’ll then use this helper to translate the 5000 most used words in both matrices. (We used an external script again, but the code ist the same.) plan(multicore(workers = 8)) german_keys &lt;- read_rds(&#39;./../data/german_stem_keys.rds&#39;) german_word_freq %&lt;&gt;% arrange(desc(n)) %&gt;% slice(1:5000) %&gt;% left_join(german_keys, by = c(&#39;word&#39; = &#39;stems&#39;)) %&gt;% transmute(word,n,long = map_chr(data,~first(unlist(.)))) %&gt;% mutate(tranlation = future_map(long, ~translate_with_mymemory(.,&#39;de&#39;),.progress = T)) german_word_freq %&gt;% write_rds(&#39;./../data/german_translated.rds&#39;, compress = &#39;gz&#39;) swedish_keys &lt;- read_rds(&#39;./../data/swedish_stem_keys.rds&#39;) swedish_word_freq %&lt;&gt;% arrange(desc(n)) %&gt;% slice(1:5000) %&gt;% left_join(swedish_keys, by = c(&#39;word&#39; = &#39;stems&#39;)) %&gt;% transmute(word,n,long = map_chr(data,~first(unlist(.)))) %&gt;% mutate(tranlation = future_map(long, ~translate_with_mymemory(.,&#39;sv&#39;), .progress = T)) swedish_word_freq %&gt;% write_rds(&#39;./../data/swedish_translated.rds&#39;, compress = &#39;gz&#39;) Let’s now see, how good the API really is (after doing some minor cleaning). clean_myMemory &lt;- function(translated_set){ translated_set %&gt;% transmute(stem = `word`, long, translation = map_chr(tranlation,~tryCatch(.$translation, error = function(e){.})), translation = str_to_lower(translation), translation = str_remove_all(translation,&#39;\\\\W&#39;), translation = str_trim(translation), n) } ger_tranlated &lt;- read_rds(&#39;./../data/german_translated.rds&#39;) %&gt;% clean_myMemory() #%T&gt;% #write_csv(&#39;data/manually_cleaned_german.csv&#39;) DT::datatable(ger_tranlated) swe_translated &lt;- read_rds(&#39;./../data/swedish_translated.rds&#39;)%&gt;% clean_myMemory() #%T&gt;% #write_csv(&#39;data/manually_cleaned_swedish.csv&#39;) DT::datatable(swe_translated) Unfortunately, the quality of the translation is quite inconsistent. To tackle that, we conducted a manual correction of the 250 most common terms resulting csv in Excel. The last step is to assign sentiment values to the keys come back to the documents and add the sentiments to our meta data-frame We will try to augment the English approach with the multilingual one but the results will be far from perfect. So let’s first add sentiments using the sentiments as indicated in the AFINN-set (Nielsen, 2011), (though it was originally intended for microblogging) and the sentiments-df from the tidytext package, which is the set as reported by Hu &amp; Liu (2004). Since the sentiments in the latter are only indicated as ‘positive’ and ‘negative,’ we need to transform this to the endpoints of the AFINN-set (-5,5). This is obviously a heavily skewed approach, but we need to accept imperfections at some point. We will use the higher resoluting AFINN-sentiments where we can though. add_sentiments &lt;- function(translated_set){ translated_set%&gt;% left_join(read_tsv(&#39;./../data/AFINN-111.txt&#39;, col_names = c(&#39;term&#39;, &#39;sentiment&#39;)), by = c(&#39;translation&#39; = &#39;term&#39;)) %&gt;% left_join(tidytext::sentiments, by = c(&#39;translation&#39; = &#39;word&#39;)) %&gt;% rename(sentiment_AFINN = sentiment.x, sentiment_tidytext = sentiment.y) %&gt;% mutate(sentiment_tidytext = recode(sentiment_tidytext, &quot;positive&quot; = 5, &#39;negative&#39; = -5), sentiment = ifelse(is.na(sentiment_AFINN), sentiment_tidytext, sentiment_AFINN)) } ger_tranlated &lt;- read_csv(&#39;./../data/manually_cleaned_german.csv&#39;) %&gt;% add_sentiments() swe_translated &lt;- read_csv(&#39;./../data/manually_cleaned_swedish.csv&#39;) %&gt;% add_sentiments() The next step is to now combine these results with the multilingual approach (after scaling the German sentiments from that approach to the -5:5 scale). Since the two sentiment measurements are bound to differ, we’ll average them wherever there are two values ger_sent %&lt;&gt;% mutate(sentiment = sentiment/max(abs(sentiment),na.rm = T) * 5) %&gt;% left_join(ger_tranlated[,c(&#39;stem&#39;,&#39;translation&#39;, &#39;sentiment&#39;)], by = c(&#39;word&#39; = &#39;stem&#39;)) %&gt;% rowwise() %&gt;% mutate(sentiment = mean(c_across(matches(&#39;sentiment&#39;)), na.rm = T)) %&gt;% select(word, translation, sentiment, n) skimr::skim(ungroup(ger_sent)) Table 3: Data summary Name ungroup(ger_sent) Number of rows 71303 Number of columns 4 _______________________ Column type frequency: character 2 numeric 2 ________________________ Group variables None Variable type: character skim_variable n_missing complete_rate min max empty n_unique whitespace word 0 1.00 4 41 0 71303 0 translation 66312 0.07 2 212 0 4934 0 Variable type: numeric skim_variable n_missing complete_rate mean sd p0 p25 p50 p75 p100 hist sentiment 70837 0.01 -0.53 2.36 -5 -2 -0.04 0.03 5 ▂▃▇▂▁ n 0 1.00 14.09 117.01 1 1 2.00 6.00 14074 ▇▁▁▁▁ swe_sent %&lt;&gt;% left_join(swe_translated[,c(&#39;stem&#39;,&#39;translation&#39;, &#39;sentiment&#39;)], by = c(&#39;word&#39; = &#39;stem&#39;)) %&gt;% rowwise() %&gt;% mutate(sentiment = mean(c_across(matches(&#39;sentiment&#39;)), na.rm = T)) %&gt;% select(word,translation, sentiment, n) skimr::skim(ungroup(swe_sent)) Table 3: Data summary Name ungroup(swe_sent) Number of rows 85174 Number of columns 4 _______________________ Column type frequency: character 2 numeric 2 ________________________ Group variables None Variable type: character skim_variable n_missing complete_rate min max empty n_unique whitespace word 0 1.00 4 34 0 85174 0 translation 80179 0.06 1 408 0 4215 0 Variable type: numeric skim_variable n_missing complete_rate mean sd p0 p25 p50 p75 p100 hist sentiment 83595 0.02 -0.35 2.43 -5 -2 -1 2 5 ▂▇▂▅▁ n 0 1.00 35.47 206.63 1 2 4 12 15223 ▇▁▁▁▁ Merging and exporting the final sets For the two visualisations we had in mind we’ll need two data sets. the overall count and sentiment per country the timecourse for each country, including sentiments and article-counts The first one is easy, we just need to combine the sentiment sets we created in the last step: imap_dfr(list(&#39;Sweden&#39; = swe_sent, &#39;Germany&#39; = ger_sent), ~mutate(.x, country = .y)) %&gt;% write_rds(&#39;./../data/wordcloud_data.rds&#39;,compress = &#39;gz&#39;) The second one is a bit more tricky, since we need to look up in the respective dtms, which document used which terms in which frequency to then average the respective sentiments. This sentiment-per-document-set can then be joined with the existing meta-data. get_sentiment_per_document &lt;- function(dtm, sentiment){ extract_counts &lt;- function(id,dtm, sentiment){ tibble(n = dtm$v[dtm$i == which(dtm$dimnames$rows == id)], term = dtm$dimnames$cols[dtm$j[dtm$i == which(dtm$dimnames$rows == id)]]) %&gt;% left_join(sentiment[,1:2], by = c(&#39;term&#39; = &#39;word&#39;)) %&gt;% mutate(partial_sent = n * sentiment/sum(n)) %$% tibble(id = id, m_sentiment = sum(sentiment, na.rm = T)) } map_dfr(dtm$dimnames$rows, ~extract_counts(., dtm, sentiment)) } swe_sent_per_doc &lt;- get_sentiment_per_document(swedish_dtm, swe_sent) ger_sent_per_doc &lt;- get_sentiment_per_document(german_dtm, ger_sent) ger_part &lt;- ger_sent_per_doc %&gt;% mutate(country = &#39;Germany&#39;) %&gt;% right_join(read_rds(&#39;./../data/german_meta.rds&#39;), by=c(&#39;id&#39; = &#39;stories_id&#39;)) %&gt;% mutate(media_id = as.numeric(media_id)) %&gt;% left_join(ger_sources[,c(&#39;media_id&#39;, &#39;pub_state&#39;)]) %&gt;% select(country, title,m_sentiment, publish_date, everything()) swe_part &lt;- swe_sent_per_doc %&gt;% mutate(country = &#39;Sweden&#39;) %&gt;% right_join(read_rds(&#39;./../data/swedish_meta.rds&#39;), by=c(&#39;id&#39; = &#39;stories_id&#39;)) %&gt;% mutate(media_id = as.numeric(media_id)) %&gt;% left_join(swe_sources[,c(&#39;media_id&#39;, &#39;pub_state&#39;)]) %&gt;% select(country, title,m_sentiment, publish_date, everything()) bind_rows(ger_part, swe_part) %&gt;% write_rds(&#39;./../data/documents_with_sentiments.rds&#39;, compress = &#39;gz&#39;) "],["vizualizing-the-data.html", "Vizualizing the data Introduction Attaining the data Viz 2 Der Plot ist noch ein bisschen überladen. Was ich nicht so richtig verstehe Viz 3", " Vizualizing the data Introduction During the COVID-19 pandemic different policies have been chosen by the countries to prevent a overloading of their health care system. Especially the country sweden has been known for choosing moderate policies. So that the question came up how the press of other countries is reporting about sweden. And in reverse, how sweden is reporting about other countries. We have choosen as a comparison Germany since it’s the country we are from. First off we want to show how the situations of new confirmed cases and the corresponding policies differ between the two countries. Attaining the data For attaining the covid-19 policy data we read the data from the github of the Coronavirus Government Response Tracker Project. limits=as.Date(c(&#39;2020-01-01&#39;, &#39;2020-12-31&#39;)) # read policies csv-file policies &lt;- read.csv(&#39;https://raw.githubusercontent.com/OxCGRT/covid-policy-tracker/master/data/OxCGRT_latest.csv&#39;) For the meanings of specific policies take a look on the Codebook of the project. A function for the data pre-filtering: # a function for pre filtering filtering &lt;- function(country_name, policy_filter) { policies_ger &lt;- policies %&gt;% filter(CountryName==country_name) %&gt;% # filter policies by germany select(policy_filter) %&gt;% # select policies of interest mutate(Date=as.Date(as.character(Date), format = &quot;%Y%m%d&quot;)) %&gt;% # change date format na.omit() %&gt;% # remove rows with na entries arrange(Date) # order rows by date } The data has to be restructured for being possible to plot. So that’s the function for doing that: # a function to build start and stop labels for the equivalent policy build_start_stop_labels &lt;- function(policies, policy, label){ # get selected policy data selected_policy &lt;- policies %&gt;% select(Date, policy) # build bit-mask for policy changes bit_mask &lt;- selected_policy[,policy] - lag(selected_policy[,policy], n = 1) != 0 # set last entry true (is na because of lag) bit_mask[is.na(bit_mask)] &lt;- TRUE # initialize start and stop list start &lt;- c() stop &lt;- c() # initialize a state variable for tracking start position start_state &lt;- TRUE # iterate through rows for finding start and stop positions for(row in 1:length(bit_mask)){ # check whether it&#39;s a stop position if(bit_mask[row] &amp; !start_state){ stop &lt;- c(stop, row-1) start_state &lt;- TRUE } # check whether it&#39;s a start position if(bit_mask[row] &amp; start_state){ start &lt;- c(start, row) start_state &lt;- FALSE } } # add last row as stop stop &lt;- c(stop, row) # retrieve start and stop dates from the data set start_date &lt;- selected_policy[start, &#39;Date&#39;] stop_date &lt;- selected_policy[stop, &#39;Date&#39;] # retrieve event number from the data set events &lt;- selected_policy[start, policy] # return dataframe with start/stop dates for the polcies return(data.frame(event = events, start = start_date, end = stop_date, group = label)) } After that we build the timeline plot for the stay at home. # set filter for columns of interest policy_filter &lt;- c(&#39;CountryCode&#39;, &#39;Date&#39;, &#39;C8_International.travel.controls&#39;, &#39;C6_Stay.at.home.requirements&#39;, &#39;C4_Restrictions.on.gatherings&#39;) # retrieve policy data for germany and sweden policies_ger &lt;- filtering(&#39;Germany&#39;, policy_filter) policies_se &lt;- filtering(&#39;Sweden&#39;, policy_filter) # rebuild data into start and stop labels timeline_data &lt;- rbind(build_start_stop_labels(policies_ger, &#39;C4_Restrictions.on.gatherings&#39;, &#39;Germany&#39;), build_start_stop_labels(policies_se, &#39;C4_Restrictions.on.gatherings&#39;, &#39;Sweden&#39;)) # determine range of events event_range &lt;- timeline_data %&gt;% select(event) %&gt;% range() # build color map depending on the range of events color_map &lt;- rev(viridis(event_range[2]+1)) # add column color and event for specific policy and corresponding policy timeline_data %&lt;&gt;% mutate(color = color_map[as.integer(event)+4]) %&gt;% mutate(event = as.character(event)) # build time line plot for the Restriction of gatherings policy time_line_plot &lt;- ggplot(timeline_data, aes(x=start, xend=end, y=group, yend=group, color=event)) + geom_segment(size=10) + scale_color_manual(labels=c(&#39;no restrictions&#39;, &#39;above 1000 people&#39;,&#39;101-1000 people&#39;, &#39;11-100 people&#39;, &#39;10 people or less&#39;), values = color_map) + labs(x = &#39;Date&#39;, y = &#39;Country&#39;, colour=&quot;Restrictions on\\n gatherings&quot;) + scale_x_date(limits = limits) After having the policy timeline for germany and sweden the time-series for the normalized new cases gets added: # retrieve covid data from covid.ourworldindata.org git repository covid_data &lt;- read.csv(&#39;https://covid.ourworldindata.org/data/owid-covid-data.csv&#39;) # filter for germany and sweden data and select location, date and smoothed new # cases per million covid_data %&lt;&gt;% filter(location==&#39;Sweden&#39; | location==&#39;Germany&#39;) %&gt;% select(location, date, new_cases_smoothed_per_million) %&gt;% # select columns of interest na.omit() %&gt;% # remove rows with na entries mutate(date=as.Date(as.character(date), format = &quot;%Y-%m-%d&quot;)) # save date in date format # define colors for time series plot COLORS &lt;- c(Germany = &quot;steelblue&quot;, Sweden =&quot;darkred&quot;) # build time-series plot lineplot &lt;- covid_data %&gt;% ggplot(aes(x = date, y = new_cases_smoothed_per_million, group = location, color = location)) + geom_line() + scale_color_manual(values = COLORS) + labs(x = &#39;&#39;, y = &#39;Smoothed New Cases\\n per\\n Million&#39;, colour=&quot;Countries&quot;) + scale_x_date(limits = limits) plot_grid(lineplot, time_line_plot, ncol = 1, align=&#39;v&#39;) Figure 1: Time-series of infections and restrictions. After cleaning the data and building with it the plot. We retrieve a subplot consisting of a time series of the smoothed new cases per million of COVID-19 for germany and sweden. In relation to that we plotted the corresponding policies germany and sweden released in respect to restrictions and gatherings. That said we want to go into how situations in germany and sweden changed and how the countries reacted to those. Especially of interrest was the time before the pandemic peak in germany in april. Both countries started on restricting gatherings. Germany reduced the allowed gatherings of 1000 people, while sweden was restricting it to 101-1000 people. After approximatly a week germany went into a lockdown, while sweden reduced the number of allowed people in a gathering to 11-100. This policy sweden kept on until now. While in Germany during late summer some attempts to lighten the policies on gatherings had been tried out. But those have been overthrown with the increasing number of new cases. This shows that Sweden chose way lighter policies with respect to gatherings than germany even with way higher number of new cases. So that we were asking ourself how this affected the narrative of the two countries to each other. The idea was to compare the news reports of the countries over each other. Viz 2 Since the crisis-response seems to be different and we got the impression of the use of sweden as a bad example in German media, we wanted to see, whether the reporting in both countries about the respective other differed as well. The first aspect we found interesting, was the amount of articles over time. sentiment_time_series &lt;- read_rds(&#39;./../data/documents_with_sentiments.rds&#39;) %&gt;% mutate(publish_date = lubridate::as_date(as.Date(publish_date))) %&gt;% group_by(country, publish_date) %&gt;% summarise(`number of articles` = n()) %&gt;% ungroup() %&gt;% group_by(country) %&gt;% group_split() %&gt;% map(~mutate(., across(where(is.numeric), ~ 100 * . / max(.)))) %&gt;% map_dfr( ~ mutate(., across( where(is.numeric), ~ stats::filter(., filter = dnorm(seq(-2, 2, length.out = 7)) / sum(dnorm( seq(-2, 2, length.out = 7) ))), .names = &#39;filter_{.col}&#39; ))) %&gt;% rename(&#39;original_number of articles&#39; = &#39;number of articles&#39;) %&gt;% pivot_longer( cols = where(is.numeric), values_to = &#39;value&#39;, names_to = c(&#39;quality&#39;, &#39;indicator&#39;), names_sep = &#39;_&#39; ) %&gt;% pivot_wider(names_from = quality, values_from = value) covid_data %&lt;&gt;% group_by(location) %&gt;% group_split() %&gt;% map_dfr(~mutate(., cases = 100 * new_cases_smoothed_per_million/max(new_cases_smoothed_per_million))) article_timeseries_viz &lt;- sentiment_time_series %&gt;% ggplot(aes(x = publish_date, y = original)) + geom_point(color = scales::muted(&#39;blue&#39;), alpha = .25) + geom_line(aes(y = filter), color = scales::muted(&#39;blue&#39;)) + facet_grid(rows = vars(country),scales = &#39;fixed&#39;) + geom_line(data = covid_data, aes(y = cases, x = date, color = location)) + scale_color_manual(values = COLORS) + theme(legend.position = &#39;bottom&#39;) + labs(x = &#39;Date of Publication&#39;, y = &#39;Number of Articles in % to Maximum&#39;, color = &#39;New Covid-19 cases:&#39;) article_timeseries_viz Figure 2: Total amount of articles per day concerning the respective country are displayed as points, the seven-day moving gaussian average of the amount is depicted by the dark blue line. The dashed lines depict the case numbers per 100.000.000 Inhabitants. All amounts are scaled, so that the relative maximum in the timeframe is set to 100% to render them comparable, since the absolute number of articles as well as the case numbers differ widely. By comparing these time-series with the infection-rates, one can recognize signs of a similarity in the number of German mentions of Sweden and Corona and the amount of cases in Sweden. Since the amount of articles seems to losely be coupled to the amount of cases in the respective other country, we will look at the content of these reports. Der Plot ist noch ein bisschen überladen. Was ich nicht so richtig verstehe Viz 3 Let’s start by using the rather patchy sentiments we were able to gather to generate a few maps sf_data &lt;- get_eurostat_geospatial(resolution = &#39;10&#39;, nuts = 3) %&gt;% filter(CNTR_CODE %in% c(&#39;SE&#39;)) %&gt;% bind_rows(get_eurostat_geospatial(resolution = &#39;10&#39;, nuts = 1) %&gt;% filter(CNTR_CODE %in% c(&#39;DE&#39;))) %&gt;% mutate(NAME_LATN = str_to_lower(NAME_LATN)) se_countrycodes &lt;- read_tsv(&#39;https://www.iso.org/obp/ui/#iso:code:3166:SE&#39;) regional_sents &lt;- read_rds(&#39;./../data/documents_with_sentiments.rds&#39;) %&gt;% group_by(country) %&gt;% group_split() %&gt;% map_dfr(~group_by(.,pub_state,country) %&gt;% summarise(sentiment = mean(m_sentiment))) Now we need to cross-reference the Mediacloud countrycodes (ISO 3166:SE, ISO 3166:DE) with the eurostat ones: iso_3166 &lt;- read_delim( &#39;Provinz;Code Blekinge län;SE-K Dalarnas län;SE-W Gotlands län;SE-I Gävleborgs län;SE-X Hallands län;SE-N Jämtlands län;SE-Z Jönköpings län;SE-F Kalmar län;SE-H Kronobergs län;SE-G Norrbottens län;SE-BD Skåne län;SE-M Stockholms län;SE-AB Södermanlands län;SE-D Uppsala län;SE-C Värmlands län;SE-S Västerbottens län;SE-AC Västernorrlands län;SE-Y Västmanlands län;SE-U Västra Götalands län;SE-O Örebro län;SE-T Östergötlands län;SE-E Baden-Württemberg;DE-BW Bayern;DE-BY Berlin;DE-BE Brandenburg;DE-BB Bremen;DE-HB Hamburg;DE-HH Hessen;DE-HE Mecklenburg-Vorpommern;DE-MV Niedersachsen;DE-NI Nordrhein-Westfalen;DE-NW Rheinland-Pfalz;DE-RP Saarland;DE-SL Sachsen;DE-SN Sachsen-Anhalt;DE-ST Schleswig-Holstein;DE-SH Thüringen;DE-TH&#39;, delim = &#39;;&#39; ) %&gt;% mutate(across(everything(), ~str_trim(.)))%&gt;% mutate(Provinz = str_to_lower(Provinz)) regional_sents %&lt;&gt;% right_join(iso_3166, by = c(pub_state = &#39;Code&#39;)) This dataset can now be used to depict the overall regional sentiment in both countries: reg_sent_plot &lt;- function(data,legend){ p &lt;- ggplot(data) + geom_sf(aes(geometry = geometry,fill = sentiment), color = &#39;grey&#39;, expand = F) + scale_fill_gradient2(low = scales::muted(&#39;red&#39;), mid = &#39;white&#39;, high = scales::muted(&#39;blue&#39;), na.value = &#39;lightgrey&#39;, limits = c(-5,5)) + cowplot::theme_map() if(!legend){ p &lt;- p + theme(legend.position = &#39;none&#39;) }else{ p &lt;- p + theme_void() + theme(legend.position = &#39;right&#39;) } p + coord_sf(crs = st_crs(&quot;+proj=merc&quot;)) } plots &lt;- sf_data %&gt;% left_join(regional_sents[,c(&#39;Provinz&#39;, &#39;sentiment&#39;)],by = c(&#39;NAME_LATN&#39; = &#39;Provinz&#39;)) %&gt;% group_by(CNTR_CODE) %&gt;% group_split() %&gt;% purrr::map(~reg_sent_plot(.,legend = F)) legend &lt;- cowplot::get_legend(reg_sent_plot(mutate(sf_data,sentiment = 0),legend = T)) cowplot::plot_grid( cowplot::plot_grid(plotlist = plots), legend, rel_widths = c(4,.4) ) Figure 3: Mean sentiment per region as detected in the mainstrem-news articles about the respective other country. And finally, let’s look at some wordclouds: wc_data &lt;- read_rds(&#39;./../data/wordcloud_data.rds&#39;) wordcloud &lt;- function(wc_data,country){ require(ggwordcloud) p &lt;- get_eurostat_geospatial(resolution = &#39;10&#39;, nuts = 0) %&gt;% filter(CNTR_CODE %in% c(country)) %&gt;% ggplot() + geom_sf(fill = &#39;black&#39;, color = &#39;black&#39;) + coord_sf(crs = st_crs(&quot;+proj=moll&quot;)) + theme_void() ggsave(&#39;temp.png&#39;, plot = p) img &lt;- png::readPNG(&#39;temp.png&#39;) dummy &lt;- colSums(img[,,1]) != max(colSums(img[,,1])) img &lt;- img[,dummy,] dummy &lt;- rowSums(img[,,1]) != max(rowSums(img[,,1])) img &lt;- img[dummy,,] wc &lt;- ggplot(wc_data,aes(label = translation, size = n, color = sentiment)) + geom_text_wordcloud(mask = img,rm_outside = T)+ scale_color_gradient2(low = scales::muted(&#39;red&#39;), mid = &#39;grey&#39;, high = scales::muted(&#39;blue&#39;), na.value = &#39;darkgrey&#39;, limits = c(-5,5)) file.remove(&#39;temp.png&#39;) wc } ger_wc &lt;- wc_data %&gt;% filter(country == &#39;Germany&#39;) %&gt;% arrange(desc(n)) %&gt;% head(500) %&gt;% wordcloud(country = &#39;DE&#39;) swe_wc &lt;- wc_data %&gt;% filter(country == &#39;Sweden&#39;) %&gt;% arrange(desc(n)) %&gt;% head(500) %&gt;% wordcloud(country = &#39;SE&#39;) cowplot::plot_grid(ger_wc, swe_wc) Figure 4: Most commonly used terms in the mainstream-news reporting about the respective other country. The color codes the sentiment if any could be associated. "],["references.html", "References", " References Allaire, J., Xie, Y., McPherson, J., Luraschi, J., Ushey, K., Atkins, A., Wickham, H., Cheng, J., Chang, W., &amp; Iannone, R. (2020). Rmarkdown: Dynamic documents for r. https://github.com/rstudio/rmarkdown Bache, S. M., &amp; Wickham, H. (2020). Magrittr: A forward-pipe operator for r. https://CRAN.R-project.org/package=magrittr Bouchet-Valat, M. (2020). SnowballC: Snowball stemmers based on the c libstemmer UTF-8 library. https://github.com/nalimilan/R.TeMiS Feinerer, I., &amp; Hornik, K. (2020). Tm: Text mining package. http://tm.r-forge.r-project.org/ Feinerer, I., Hornik, K., &amp; Meyer, D. (2008). Text mining infrastructure in r. Journal of Statistical Software, 25(5), 1–54. https://www.jstatsoft.org/v25/i05/ Hester, J. (2020). Glue: Interpreted string literals. https://CRAN.R-project.org/package=glue Hornik, K., Meyer, D., &amp; Buchta, C. (2020). Slam: Sparse lightweight arrays and matrices. https://CRAN.R-project.org/package=slam Hu, M., &amp; Liu, B. (2004). Mining and summarizing customer reviews. Proceedings of the Tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 168–177. https://doi.org/10.1145/1014052.1014073 Lahti, L., Huovari, J., Kainu, M., &amp; Biecek, P. (2017). Eurostat r package. In R Journal (No. 1; Vol. 9, pp. 385–392). https://journal.r-project.org/archive/2017/RJ-2017-019/index.html Lahti, L., Huovari, J., Kainu, M., &amp; Biecek, P. (2020). Eurostat: Tools for eurostat open data. https://ropengov.github.io/eurostat/ Nielsen, F. Å. (2011). A new ANEW: Evaluation of a word list for sentiment analysis in microblogs. arXiv:1103.2903 [cs]. http://arxiv.org/abs/1103.2903 Nusko, B., Tahmasebi, N., &amp; Mogren, O. (2016). Building a sentiment lexicon for Swedish. Digital Humanities 2016. From Digitization to Knowledge 2016: Resources and Methods for Semantic Processing of Digital Works/Texts, Proceedings of the Workshop, July 11, 2016, Krakow, Poland, 32–37. Pebesma, E. (2018). Simple Features for R: Standardized Support for Spatial Vector Data. The R Journal, 10(1), 439–446. https://doi.org/10.32614/RJ-2018-009 Pebesma, E. (2020). Sf: Simple features for r. https://CRAN.R-project.org/package=sf Qiu, Y., &amp; See file AUTHORS for details., authors/contributors of the included software. (2020). Showtext: Using fonts more easily in r graphs. https://github.com/yixuan/showtext R Core Team. (2020). R: A language and environment for statistical computing. R Foundation for Statistical Computing. https://www.R-project.org/ Remus, R., Quasthoff, U., &amp; Heyer, G. (2010). SentiWS-A Publicly Available German-language Resource for Sentiment Analysis. LREC. Vaughan, D., &amp; Dancho, M. (2020). Furrr: Apply mapping functions in parallel using futures. https://github.com/DavisVaughan/furrr Wickham, H. (2019). Tidyverse: Easily install and load the tidyverse. https://CRAN.R-project.org/package=tidyverse Wickham, H. (2020). Httr: Tools for working with URLs and HTTP. https://CRAN.R-project.org/package=httr Wickham, H., Averick, M., Bryan, J., Chang, W., McGowan, L. D., François, R., Grolemund, G., Hayes, A., Henry, L., Hester, J., Kuhn, M., Pedersen, T. L., Miller, E., Bache, S. M., Müller, K., Ooms, J., Robinson, D., Seidel, D. P., Spinu, V., … Yutani, H. (2019). Welcome to the tidyverse. Journal of Open Source Software, 4(43), 1686. https://doi.org/10.21105/joss.01686 Wilke, C. O. (2020). Cowplot: Streamlined plot theme and plot annotations for ggplot2. https://wilkelab.org/cowplot/ Xie, Y. (2014). Knitr: A comprehensive tool for reproducible research in R. In V. Stodden, F. Leisch, &amp; R. D. Peng (Eds.), Implementing reproducible computational research. Chapman; Hall/CRC. http://www.crcpress.com/product/isbn/9781466561595 Xie, Y. (2015). Dynamic documents with R and knitr (2nd ed.). Chapman; Hall/CRC. https://yihui.org/knitr/ Xie, Y. (2016). Bookdown: Authoring books and technical documents with R markdown. Chapman; Hall/CRC. https://github.com/rstudio/bookdown Xie, Y. (2020a). Bookdown: Authoring books and technical documents with r markdown. https://github.com/rstudio/bookdown Xie, Y. (2020b). Knitr: A general-purpose package for dynamic report generation in r. https://yihui.org/knitr/ Xie, Y., Allaire, J. J., &amp; Grolemund, G. (2018). R markdown: The definitive guide. Chapman; Hall/CRC. https://bookdown.org/yihui/rmarkdown Xie, Y., Dervieux, C., &amp; Riederer, E. (2020). R markdown cookbook. Chapman; Hall/CRC. https://bookdown.org/yihui/rmarkdown-cookbook Zhu, H. (2020). kableExtra: Construct complex table with kable and pipe syntax. https://CRAN.R-project.org/package=kableExtra "]]
